%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT CLASS AND GENERAL OPTIONS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[
	parskip, 			   % german paragraph style (no indentation, blank line)
	twoside, 			   % two-sided document
	DIV=14, 			   % narrow margin
	BCOR=15.0mm, 		   % binding correction
	headsepline, 		   % header line
	open=right, 		   % new chapters start on right hand side
	captions=tableheading, % correct distance between table and caption above
	bibliography=totoc,    % references shown in contents
	numbers=noenddot       % remove dots at the the end of chapter numbers
]{scrreprt}

\newcommand{\languages}{english}

\makeatletter
\newcommand\addcase[3]{\expandafter\def\csname\string#1@case@#2\endcsname{#3}}
\newcommand\makeswitch[2][]{%
	\newcommand#2[1]{%
		\ifcsname\string#2@case@##1\endcsname\csname\string#2@case@##1\endcsname\else#1\fi%
	}%
}
\makeatother

\makeswitch[nada]\dothis
\addcase\dothis{english}{\include{prechapters/declaration}}
\addcase\dothis{ngerman}{\include{prechapters/Eigenstaendigkeitserklaerung}}


%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGES AND SETTINGS %
%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} % input umlaut characters directly
\usepackage[T1]{fontenc} % proper output of umlaut characters
\usepackage[ngerman, \languages]{babel} % hyphenation, main language in the end
\usepackage{scrlayer-scrpage} % customise head and foot
\automark[chapter]{chapter} % display chapter name on all pages instead of section name on right hand side
\usepackage{color} % definition of own colors
\definecolor{LTD_grey}{gray}{0.65} % LTD Grau nach dem neuen, druckkostensparenden caltechgray
\addtokomafont{pagehead}{\color{LTD_grey} \sffamily \upshape} % head in grey and sans serif
\addtokomafont{pagenumber}{\color{LTD_grey}} % page number in grey
\addtokomafont{subject}{\normalfont} % Thesis type not bold
\usepackage{graphicx} % scaling of graphics
\usepackage{booktabs} % lines for tabs
\usepackage{amssymb,amsmath} % math symbols
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}


%%%%%%%%%%%
% CONTENT %
%%%%%%%%%%%
\begin{document}
\pagenumbering{roman} % roman numbering before main part


%%%%%%%%%%%%%%%%%%%%
% TITLE AND AUTHOR %
%%%%%%%%%%%%%%%%%%%%
\title{On the identification of port-Hamiltonian models via machine-learning} % title
\subject{Master's thesis} % type (Bachelor's thesis / Master's thesis / project thesis)
\author{Jiandong Zhao} % author
 \date{October 2022} % period
 \publishers{supervised by\\[1em]
	Prof. Dr.-Ing. habil. S. Leyendecker\\
	M. Sc. Markus Lohmayer % name of supervisor
	}
\titlehead{\includegraphics[height=1.45cm]{figures/uni_head_new} \hfill \includegraphics[height=1.3cm]{figures/ltd_head}}

% title page
\maketitle


%%%%%%%%%%%%%%%%%%%%%%
% CONTENTS AND LISTS %
%%%%%%%%%%%%%%%%%%%%%%
%% declaration
\dothis{\languages}

% abstract
% \include{prechapters/abstract}

% contents
% \tableofcontents

% figure list
% \listoffigures

% table list
% \listoftables

\cleardoublepage % finish current page with roman numbering


%%%%%%%%%%%%%%%%
% MAIN CONTENT %
%%%%%%%%%%%%%%%%
\pagenumbering{arabic} % arabic numbering for main part


% chapters
\chapter{Introduction}
\section{Motivation}
\section{Main Contributions}
\section{Outline}


\clearpage
\chapter{Differential Equation}

\section{Ordinary Differential Equation}
Consider a real-valued function $x$ with $k$ continuous derivatives: $x \in C^{k}(X), \: X \subseteq \mathbb{R}, \: k \in \mathbb{N}_{0}, \: x: \mathbb{R} \rightarrow \mathbb{R}$. An implicit ordinary differential equation (ODE) is of the form:

\begin{equation}
    \label{eq:implicit_ODE}
    f\left(t, x, x^{(1)}, \ldots, x^{(k)}\right)=0.
\end{equation}

Corresponding to implicit form, an explicit ordinary differential equation is of the form:

\begin{equation}
    \label{eq:explicit_ODE}
    x^{(k)} = f\left(t, x, x^{(1)}, \ldots, x^{(k-1)}\right).
\end{equation}

Suppose the function $x: \mathbb{R} \rightarrow \mathbb{R}^{n}$. Then the equation \ref{eq:explicit_ODE} should be rewritten in the form of system:

\begin{equation}
    \label{eq:ODE_system}
    \begin{aligned}
    x_{1}^{(k)} &=f_{1}\left(t, x, x^{(1)}, \ldots, x^{(k-1)}\right) \\
    x_{2}^{(k)} &=f_{2}\left(t, x, x^{(1)}, \ldots, x^{(k-1)}\right) \\
    & \vdots \\
    x_{n}^{(k)} &=f_{n}\left(t, x, x^{(1)}, \ldots, x^{(k-1)}\right).
\end{aligned}
\end{equation}

The above form is a N-dimensional system, which has one or more ordinary differential equations. It is commonly called t as the independent variable and x as the dependent variable.

\section{Initial Value Problem}
A initial value problem(IVP) consists of a pair of equations:

\begin{equation}
    \label{eq:IVP}
    \dot{x} = f(t, x), \quad x(t_{0})=x_{0} 
\end{equation}

Then the integral of the initial value problem is of the form:

\begin{equation}
    \label{eq:solution_IVP}
    x(t) = x_{0} + \int_{t_{0}}^{t} f(s, x(s))\:ds.
\end{equation}

Suppose $t$ is infinite small such that $x_{0}(t)=x_{0}$. Interpolate it to the equation \ref{eq:solution_IVP}, the next state $x_{1}$ after the initial state $x_{0}$  can be written as:

\begin{equation}
    \label{eq:solution_IVP_first_step}
    x_{1}(t) = x_{0} + \int_{t_{0}}^{t} f(s, x_{0}(s))\:ds.
\end{equation}

By repeating this procedure, it is not difficult to deduce

\begin{equation}
    \label{eq:solution_IVP_steps}
    x_{m+1}(t) = x_{0} + \int_{t_{0}}^{t} f(s, x_{m}(s))\:ds.
\end{equation}

for all approximating states.

TODO: more details


\clearpage
\section{Numerical Method}
To eliminates the complicated procedures of solving differential equations (the procedure in \ref{eq:solution_IVP_steps}), some numerical methods such as Euler's method are common to use.
\subsection{Euler Method}
Euler's method: Consider a IVP problem in equations \ref{eq:IVP}. Let $t_{m}=t_{0}+hm$, where h is the time step size.

\begin{equation}
    \label{eq:Eulers_method}
    y_{n+1} = y_{n} + h \cdot f_{n},
\end{equation}

where h is the steps interval $t_{n+1} - t_{n}$ and $f_{n}$ is the derivative of $y$ at the point $t_{n}$, i.e. $f_{n} = f(y_n, t_n) = \left. \frac{dy}{dt} \right|_{t=t_n}$
\subsection{Symplectic Euler Method}
\subsection{Implicit Midpoint Rule}


\clearpage
\chapter{Hamiltonian Systems}
\section{Hamiltonian Systems}
A Hamiltonian system is a triple $(M,\omega,H)$, where $(M,\omega)$ is a symplectic manifold which consists of a manifold $M$ and a symplectic structure $\omega$. The preservation of the $\omega$ preserves the Hamiltonian during the evolution of the system.

The Hamiltonian or Hamiltonian function $H$ is a smooth function on the manifold $M$ that assigns the coordinates to conserved quantities of physical systems \cite{rudolph2017differential}. Normally, the Hamiltonian of an isolated physical system represents the total energy. For instance, in a mechanical system with k state variables $q_{i}$, the total energy is $E(\mathbf{q}) = E(q_{1}, . . . ,q_{k})$, where the state variables are some natural variables. With the generalized coordinate $\mathbf{q}$ and generalized momentum $\mathbf{p}$, it holds that

\begin{equation}
    \label{eq:Hamiltonian}
    E(\mathbf{q})=H(\mathbf{q},\mathbf{p})=T+V=\frac{1}{2}\mathbf{p}^T\mathbf{M}^{-1}(\mathbf{q})\mathbf{p} + \mathbf{V}(\mathbf{q}),
\end{equation}

where $T$ is kinetic energy and $V$ is potential energy of the system.

Let the state variables $\mathbf{x}=(\mathbf{q},\mathbf{p}) \in \mathbb{X}$. Since the Hamiltonian is invariant with respect to time, it holds

\begin{equation}
    \label{eq:Hamiltonian_invariant}
    \frac{dH}{dt} = (\frac{\partial H}{\partial x})^T \frac{dx}{dt} = 0,
\end{equation}

where $\frac{dx}{dt}$ can be written in the form:

\begin{equation}
    \label{eq:Hamiltonian_symplectic}
    \begin{aligned}
        \frac{dx}{dt} &= J \frac{\partial H}{\partial x},\\\\
        \text{where}\\
        J &= \left[ \begin{array}{cc}
            0 & -I \\
            I & 0
        \end{array} \right]
    \end{aligned}
\end{equation}

such that 

\begin{equation}
    \label{eq:Hamiltonian_symplectic_conclusion}
    \begin{aligned}
        \frac{dH}{dt} &= (\frac{\partial H}{\partial x})^T \frac{dx}{dt}\\
         &= (\frac{\partial H}{\partial x})^T J \frac{\partial H}{\partial x} \\
         &= 0.
    \end{aligned}
\end{equation}

A matrix in the form of $J$ is called skew-symmetry matrix.

\section{Port-Hamiltonian Systems}
A port-Hamiltonian system is a Hamiltonian system endowed with a geometric structure, which is known as Dirac structure. A Dirac structure is a subspace $\mathcal{D} \subset \mathcal{F} \times \mathcal{E}$, where $\mathcal{F}$ and $\mathcal{E}$ are the spaces of the port variables (i.e. flows $f$ and efforts $e$), such that for all pair $(f,e)$ in the Dirac structure $\mathcal{D}$ the power $<e \mid f>$ equal to zero (Vincent section 2.1.2). Note that $\mathcal{E}$ is the dual space of $\mathcal{F}$.

The concept of "ports" comes from the port-based physical system modeling approach (Vincent). It reveals that subsystems interact with each other via ports. In port-Hamiltonian systems, such interactions are assumed to be exchanges of energy.


TODO: more details
\section{Dynamic Simulation with Numerical Method}
\subsection{Undamped Harmonic Oscillator}
In the classical simple undamped harmonic oscillator model, a mass $m$ is fixed to one end of a spring with spring compliance $c$ and the other end of the spring is fixed to a fixed rigid body. The spring displacement $q$ is in the positive direction of the direction of stretching by the mass. See figure \ref{fig:physical_model_undamped_harmonic_oscillator}.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/undamped harmonic oscillator.pdf}
    \caption{Undamped harmonic oscillator}
    \label{fig:physical_model_undamped_harmonic_oscillator}
\end{figure}

According to Newton's second law $F=ma=m\ddot{x}$ and Hooke's law $F=-kx$, it holds $m\ddot{x}+kx=0$, which is an implicit ODE in the form of the equation \ref{eq:implicit_ODE}. As the highest order in the ODE is second order, it is called second order differential equation.

In the undamped harmonic oscillator model, referring to the equation \ref{eq:Hamiltonian}, the Hamiltonian is the sum of the kinetic energy $T=\frac{1}{2m}p^2$ and the potential energy $\frac{1}{2c}q^2$:

\begin{equation}
    \label{eq:Hamiltonian_udho}
    H(q,p)=\frac{1}{2c}q^2+\frac{1}{2m}p^2,
\end{equation}

where p is the momentum of the mass. The state of the system (also known as canonical coordinates) $x=(q,p)$ moves along the symplectic gradient $\dot{x}=(\dot{q},\dot{p})$:

\begin{equation}
    \label{eq:symplectic_gradient}
    \dot{q}=\frac{\partial H}{\partial p}, \quad \dot{p}=-\frac{\partial H}{\partial q}.
\end{equation}

Moving along the symplectic gradient holds the total energy of the conserve system, i.e. the Hamiltonian, constant. So that the derivative of the Hamiltonian in Equation \ref{eq:derivative_Hamiltonian} is hold as zero.

\begin{equation}
    \label{eq:derivative_Hamiltonian}
    \dot{H}=\frac{\partial H}{\partial q}\dot{q}+\frac{\partial H}{\partial p}\dot{p}=0
\end{equation}

Referring to the equations \ref{eq:ODE_system}, the ordinary differential equation system of the undamped harmonic oscillator could be written as
\begin{equation}
    \label{eq:ODE_undamped_harmonic_oscillator}
    \begin{bmatrix}
    \dot{q}\\
    \dot{p}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \frac{p}{m}\\
    -\frac{q}{c}
    \end{bmatrix}.
\end{equation}

\subsection{Isothermal Damped Harmonic Oscillator}
In contrast to undamped harmonic oscillator, the isothermal damped harmonic oscillator should be regarded as a port-Hamiltonian system.

TODO: more details

The ordinary differential equation system of the isothermal damped harmonic oscillator are written as

\begin{equation}
    \label{eq:ODE_isothermal_damped_harmonic_oscillator}
    \begin{bmatrix}
    \dot{q}\\
    \dot{p}\\
    \dot{s_{e}}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \frac{p}{m}\\
    -\frac{q}{c}-d\frac{p}{m}\\
    d(\frac{p^2}{m^2})\frac{1}{\theta_{o}}
    \end{bmatrix},
\end{equation}

where $d$ is the damping coefficient, $s_{e}$ is the entropy of the environment, $\theta_{o}$ is the environmental temperature.

To investigate the dynamics of the physical system, it is common to simulate the trajectories of the dynamical system with respect to time. The following block diagram illustrates this process.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/dynamic simulation.pdf}
    \caption{The process of dynamic simulation}
    \label{fig:simulation}
\end{figure}

For simplicity, consider an undamped harmonic oscillator model. The Hamiltonian on the left in figure \ref{fig:simulation} is $H(q,p)$ in equation \ref{eq:Hamiltonian_udho}. The next block of automatic differentiation (AD) stands for a set of techniques that allow a computer program to yield the derivative of a function. Automatic differentiation uses the fact that any computer program that implements a vector-valued function could be decomposed into a sequence of basic specified operations, each of which can be easily differentiated by looking up a table. These basic partial derivatives that compute a particular term are combined into a differential form, such as gradient, Jacobian, etc. In the case of simple undamped harmonic oscillator, the output of the automatic differentiation block is the symplectic gradient $\dot{x}=(\dot{q},\dot{p})$ in the equation \ref{eq:symplectic_gradient}. In fact, this symplectic gradient is consistent with the form of a 2-dimensional ordinary differential equations system mentioned in the equation \ref{eq:ODE_system}, also see the equation \ref{eq:ODE_undamped_harmonic_oscillator}. This ODE system together with initial condition $x_{0}$ and the independent variable $t$ specifies an IVP problem (also called ODE problem in the block diagram). To evaluate the trajectories of the states, or in other words, to integrate the ODEs, it is efficient to introduce numerical methods to solve this ODE problem. For instance, the integrator block may embed a fourth order symplectic Rungeâ€“Kutta method, which integrates the symplectic 2-form $dq \wedge dp$ over a symplectic manifold. The solution of the integrator would be the time evolution of the canonical coordinates $x(t)=\left( q(t), p(t) \right)$ with respect to time in the figure \ref{fig:simulation_udho}.

\clearpage
\begin{figure}[h!]
    \centering
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[scale=0.8]{figures/udho_time_evolution.png}
        \caption{Time evolution of the canonical coordinates}
        \label{fig:simulation_udho}
    \end{subfigure}
    \vspace{1em}
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[scale=0.8]{figures/udho_time_evolution_Hamiltonian.png}
        \caption{Time evolution of the Hamiltonian}
        \label{fig:Hamiltonian_udho}
    \end{subfigure}
    \caption{Time evolution of a simple undamped harmonic oscillator model}
    \label{fig:time_evolution_udho}
\end{figure}

In the figure \ref{fig:Hamiltonian_udho}, the Hamilton is as time-invariant as one might imagine. Thus, the canonical coordinates in phase space would be an ellipse in the figure \ref{fig:phase_portrait_udho}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.8]{figures/phase_portrait_canonical_coordinates.png}
    \caption{Phase portrait of a simple undamped harmonic oscillator model}
    \label{fig:phase_portrait_udho}
\end{figure}


\clearpage
\chapter{Neural Networks}
Artificial neural networks (ANNs), also referred to as neural networks (NNs), are algorithmic mathematical models that mimic the behavioral characteristics of animal neural networks for distributed parallel information processing, even though this mimicry is superficial \cite{russell2010artificial}. Such networks rely on the complexity of the system to process information by adjusting the relationship between a large number of internal nodes connected to each other.

\section{Perceptron}
Neural network technology originated in the 1950s as perceptron by McCulloch and Pitts. The characteristics of perceptrons are strongly contemporary: their inputs and outputs are in binary form. Each of these perceptrons is characterized as either "on" or "off", with an "on" response occurring when stimulated by a sufficient number of neighboring perceptrons.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/perceptron.pdf}
    \caption{Perceptron}
    \label{fig:perceptron}
\end{figure}

In the figure \ref{fig:perceptron}, $x$ denotes multiple inputs to the perceptron, $w$ denotes the weight corresponding to each input and the arrow to the right of the perceptron indicates that it has only one output. Each input is multiplied by the corresponding weight and then summed, and the result is compared with a threshold, with 1 being output if it is greater than the threshold and 0 being output if it is less than the threshold:

\begin{equation}
    \label{eq:perceptron_1}
    f(x)=\begin{cases}0 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i} \leq \text{threshold} \\ 1 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i}>\text{threshold}\end{cases}
\end{equation}

Let $b=-\text{threshold}$, the formula \ref{eq:perceptron_1} can be rewritten as:

\begin{equation}
    \label{eq:perceptron_2}
    f(x)=\begin{cases}0 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i}+b \leq 0 \\ 1 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i}+b>1\end{cases},
\end{equation}

where b is also known as bias. 

\section{Activation Functions}
Following the designers of the perceptron, McCulloch and Pitts, a node in neural network computes the weighted sum of inputs and then applies a activation function to yield the output $g(z)$, where $z=\sum\nolimits_{i=1}^n w_{i} x_{i} + b$. For instance, the perceptron in the equation \ref{eq:perceptron_2} applied the Heaviside step function (also known as binary step function) 

\begin{equation}
    \label{eq:activation_function}
    g(z)=\begin{cases} 0 & \text{if } z\leq 1 \\ 1  & \text{if } z>0
    \end{cases}
\end{equation}

as its activation function.

The figure \ref{fig:neuron_structure} shows the relationship between perceptron and activation function.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/activation_function.pdf}
    \caption{The structure of a classical neuron}
    \label{fig:neuron_structure}
\end{figure}

A good activation function should be continuous and differentiable, such that the derivative of the activation function can be learned directly by using numerical optimization methods for updating parameters. In addition, the domain of derivatives should be in a suitable range, not too large or too small. Otherwise, it will affect the efficiency and stability of the training. Despite some of the limitations mentioned above, there are still a wide variety of activation functions available. One of them is logistic sigmoid:

\begin{equation}
    \label{eq:sigmoid}
    \sigma(z)=\frac{1}{1+\exp (-z)}.
\end{equation}

Sigmoid is a classical saturating function. It is common to use sigmoid as activation function when the output is expected to be probabilistic. Since the probability in the real world is always limited to the range of 0 to 1, this is consistent with the range of sigmoid.

TODO: plot sigmoid

Another activation function similar to the sigmoid function is the tanh function:

\begin{equation}
    \label{eq:tanh}
    f(z)=\frac{\left(e^{z}-e^{-z}\right)}{\left(e^{z}+e^{-z}\right)}
\end{equation}

Compared to sigmoid, tanh function has the output range of -1 to 1:

TODO: plot tanh

One characteristic of the tanh function is zero-centered, while the sigmoid function is non-zero-centered. The non-zero-centered output causes a bias shift in the input of the neurons in the later layer and further slows down the convergence of gradient descent. That is the reason why the tanh function converges faster than sigmoid.


The ReLU function (Rectified Linear Unit):

\begin{equation}
    \label{eq:ReLU}
    ReLU(x) = max(0,x).
\end{equation}

Neurons with ReLU only need to perform addition, multiplication and comparison operations, which is more efficient in computation. However, the ReLU function is still non-zero-centered, which may affect the efficiency of gradient descent. Furthermore, the use of ReLU activation functions may lead to the well-known dying ReLU problem. One may need to try other functions in ReLU family to avoid this problem.

TODO: plot ReLU


\section{Neural Networks Architecture}
So far, neural networks have evolved a variety of architectures. The following three types of neural network architectures are commonly used: feedforward neural network, feedback neural network and graph neural network.
Take one of the simplest neural network models as an example: a feedforward neural network could be seen as a directed acyclic graph (DAG) with specified input and output nodes, which are fully connected to each other, i.e., the nodes in the later layer are all connected to each node in the former layer. Each node computes its input from the former layer with the parameters and activation function and passes the result to the nodes in the latter layer as their inputs. By definition of a directed acyclic graph, these nodes will never form a closed loop. The figure \ref{fig:feedforward} is an example of feedforward neural network.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/feedforward_neural_network.pdf}
    \caption{Architecture of a feedforward neural network.}
    \label{fig:feedforward}
\end{figure}

To clarify the principle of feedforward neural network, there are some notations to declare first. The uppercase $L$ stands for the number of total layers (input layer not included) in the feedforward neural network, the lowercase $l$ for the $l$th layer and $M_{l}$ for the number of neurons in $l$th layer. The same as the equation \ref{eq:activation_function}, $g_{l}(z^{(l)})$ denotes the activation function in $l$th layer, where $z^{l} \in \mathbb{R}^{M_{l}}$. The parameters weight and bias are represented by the weight matrix $\mathbf{W}^{(l)} \in \mathbb{R}^{M_{l} \times M_{l-1}}$ and the bias matrix $\mathbf{b}^{(l)} \in \mathbb{R}^{M_{l}}$. Thus, the input $\mathbf{z}^{(l)}$ and output $\mathbf{a}^{(l)}$ of a neuron in $l$th layer could be written as:

\begin{equation}
    \label{eq:input_output_neuron}
    \begin{aligned}
    \mathbf{z}^{(l)} &= \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}, \quad \mathbf{z}^{(l)} \in \mathbb{R}^{M_{l}}\\
    \mathbf{a}^{(l)} &= g_{l}(\mathbf{z}^{(l)}), \quad \mathbf{a}^{(l)} \in \mathbb{R}^{M_{l}}.
    \end{aligned}
\end{equation}

In fact, the whole feedforward neural network can be seen as a function $f(\mathbf{x};\mathbf{\theta})$, where $\mathbf{x}$ is the input of the neural network and $\mathbf{\theta}$ is the set of the parameters (including weight and bias). Note that the input $\mathbf{x}$ here is the output of the neurons in the input layer, i.e., $\mathbf{x}=[x_1, ..., x_n]=\mathbf{a}^{(0)}$ and the function itself is the output of the neurons in the ouput layer, i.e., $f(\mathbf{x};\mathbf{\theta})=\mathbf{a}^{(L)}$. In general, one can use the feedforward propagation algorithm as the first step to yield initial output to compute the loss function for further optimization. The following is the algorithmic procedure for feedforward propagation:


\clearpage
\begin{algorithm}[h!]
\caption{The feedforward propagation algorithm}
\label{alg:feedforward_alg}
    \begin{algorithmic}
    \Require neural network $f(\mathbf{x};\mathbf{W},\mathbf{b})$ with $l$th layer
    \Require weight matrix $\mathbf{W} \in \mathbb{R}^{M_{l} \times M_{l-1}}$, bias matrix $\mathbf{b} \in \mathbb{R}^{M_{l}}$
    \Require activation function $g(z)$
    \Require input $\mathbf{x}$
    \State $\mathbf{a}^{(0)}=\mathbf{x}$ \Comment{The output of the input layer}
    \For{$n=1...N$}
        \For{$l=1...L$}
            \State $\mathbf{z}^{(l)} \gets \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$ \Comment{Compute the input of a neuron in $l$th layer}
            \State $\mathbf{a}^{(l)} \gets g_{l}(\mathbf{z}^{(l)})$  \Comment{Compute the output of a neuron in $l$th layer}
            \State \Comment{See the equation \ref{eq:input_output_neuron}}
        \EndFor
    \EndFor
    \State \Return $f(\mathbf{x};\mathbf{W},\mathbf{b})=\mathbf{a}^{(L)}$ \Comment{Return the initial output}
    \end{algorithmic}
\end{algorithm}

\section{Optimization}
An optimization algorithm is ordinarily an algorithm that seeks the optimum. In general, finding the global optimum on a convex objective function is the simplest problem. However, if the objective function is unfortunately a non-convex function, then one has to settle for second best, i.e., seeking the local optimum.

\subsection{Data Set}
A set of samples (or data) is called a data set. In machine learning, there are two types of data set: training set and test set. The training set is used to train the model, while the test set is used to evaluate the model. Such a data set (whether for training or for testing) can be denoted by $\mathcal{D}=\{ (\mathbf{x}_{n}, \mathbf{y}_{n}) \}_{N}^{n=1} = \{ (\mathbf{x}_{1}, \mathbf{y}_{1}),  (\mathbf{x}_{2}, \mathbf{y}_{2}), ..., (\mathbf{x}_{N}, \mathbf{y}_{N}) \}$.

\subsection{Loss Functions}
The loss function $\mathcal{L}\left( y,f(\mathbf{x};\mathbf{\theta}) \right)$ is a non-negative real-valued function that quantifies the error between the target function $y$ and the model prediction $f(\mathbf{x};\mathbf{\theta})$. A small error is always expected, as it implies the prediction is very close to the truth. The following are three commonly used loss functions: zero-one loss function, absolute error loss function and squared error loss function.

Zero-one loss function:

\begin{equation}
    \label{eq:zero_one_loss}
    \mathcal{L}_{0/1}\left( y,f \left(\mathbf{x};\mathbf{\theta} \right) \right) = \begin{cases} 0 & \text{if } y=f(\mathbf{x};\mathbf{\theta}) \\ 1 & \text{if } y \neq f(\mathbf{x};\mathbf{\theta})
    \end{cases}
\end{equation}

The zero-one loss function is pretty intuitive for judging whether the result is good or bad. However, the binary output does not provide some room for further optimization.

Absolute error loss function, also called $\mathcal{L}_{1}$ loss:

\begin{equation}
    \label{eq:AE_loss}
    \mathcal{L}_{1}\left( y,f(\mathbf{x};\mathbf{\theta}) \right) = |y-f(\mathbf{x};\mathbf{\theta})|
\end{equation}

Squared error loss function, also called $\mathcal{L}_{2}$ loss:

\begin{equation}
    \label{eq:SE_loss}
    \mathcal{L}_{2}\left( y,f(\mathbf{x};\mathbf{\theta}) \right) = (y-f(\mathbf{x};\mathbf{\theta}))^2
\end{equation}

The $\mathcal{L}_{1}$ and $\mathcal{L}_{2}$ loss are widely used in linear regression models to evaluate the error. In general, the $\mathcal{L}_{2}$ loss is preferred since it is more sensitive. Yet the $\mathcal{L}_{1}$ loss performs better than the $\mathcal{L}_{2}$ loss in terms of robustness, especially when there are some outliers in the data.

In addition to the mentioned loss function, it is worth noting one more concept in statistics: the mean squared error (MSE). In machine learning, the following errors are commonly used as the target function for the optimization:

\begin{equation}
    \label{eq:MSE}
    MSE = \frac{1}{N} \sum\nolimits_{n=1}^N \mathcal{L}_{2}\left( y_n,f(\mathbf{x}_n;\mathbf{\theta}) \right) = \frac{1}{N} \sum\nolimits_{n=1}^N (y_n-f(\mathbf{x}_n;\mathbf{\theta}))^2
\end{equation}
\subsection{Gradient Descent}
Gradient Descent (GD) is the most usual optimization algorithm in machine learning. The idea is to iterate the following procedure:

\begin{equation}
    \label{eq:GD}
    \theta_{t+1} = \theta_{t} - \alpha \frac{1}{N} \sum\nolimits_{n=1}^N \nabla \mathcal{L}\left( y_{n},f(\mathbf{x_{n}};\mathbf{\theta}) \right)
\end{equation}

where $\theta_{t}$ is the parameters at the time step $t$, $\alpha$ is the learning rate and $\nabla \mathcal{L}\left( y^{(n)},f(\mathbf{x^{(n)}};\mathbf{\theta}) \right)$ is the gradient of the loss function:

\begin{equation}
    \label{eq:Gradient_loss}
    \nabla \mathcal{L}\left( y_{n},f(\mathbf{x_{n}};\mathbf{\theta}) \right) = \frac{\partial \mathcal{L}\left( y_{n},f(\mathbf{x_{n}};\mathbf{\theta}) \right)}{\partial \theta}
\end{equation}

The parameters $\theta_{t}$ in each iteration are implemented to the neural network model, and the error (e.g. the mean squared error in the equation \ref{eq:MSE}) is also updated simultaneously with the update of the neural network model. Once the error is small enough to be a desirable value, then the model is considered to be well trained.

\subsection{Stochastic Gradient Descent}
In the gradient descent \ref{eq:GD} above, the target function is the MSE \ref{eq:MSE} over the whole training set, and this approach is called Batch Gradient Descent (BGD).  The Batch Gradient Descent algorithm requires computing the gradient of the loss function for each samples in each iteration. When the size of the training set is large, the space complexity is high and the computational overhead of each iteration is also large.

In order to reduce the computational complexity, it is also possible to pick only one sample in each iteration, compute the gradient of the loss function for this sample and update the parameters. This approach is known as Stochastic Gradient Descent (SGD). For the optimization problems involving non-convex objective function, due to adding the stochastic feature, the SGD algorithm may escape from local optima easier and converge faster than BGD.

\subsection{Backpropagation}
Backpropagation is an algorithm for efficient computation of gradients. The kernel of this algorithm lies in the computation of the partial derivatives of the loss function with respect to parameters, i.e., the gradient in the equation \ref{eq:Gradient_loss}. Since this computation requires the help of automatic differentiation techniques to improve computational efficiency, backpropagation is also known as reverse mode automatic differentiation \cite{baydin2018automatic}.

Consider a neural network $f(\mathbf{x};\mathbf{\theta})=f(\mathbf{x};\mathbf{W},\mathbf{b})$. Based on the chain rule, the partial derivatives of the loss function could be written as the form:

\begin{equation}
    \label{eq:Chain_rule_partial}
    \begin{aligned}
        \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{W}^{(l)}} &= \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{z}^{(l)}} 
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}}
        \\
        \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{b}^{(l)}} &= \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{z}^{(l)}}
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}}
    \end{aligned}
\end{equation}

where $\mathbf{z}^{(l)}$ is the input of a neuron in $l$th layer, which is already mentioned above in the equation \ref{eq:input_output_neuron}, i.e., $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$. Notice that in the equation \ref{eq:Chain_rule_partial} only three terms to be computed: $\frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}}$, $\frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}}$ and $\frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{z}^{(l)}}$.

The first two terms are relatively simple to deduce:

\begin{equation}
    \label{eq:two_terms}
    \begin{aligned}
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}} &= \mathbf{a}^{(l-1)}
        \\
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}} &= \mathbf{I},
    \end{aligned}
\end{equation}

where $\mathbf{I}$ is an identity matrix. The third term is called the error, which reflects the sensitivity of the loss to neurons in $l$th layer. It is denoted by $\delta^{(l)}$. Applying the chain rule, the error $\delta^{(l)}$ in terms of the error in the
later layer $\delta^{(l+1)}$ can be written as:

\begin{equation}
    \label{eq:error_in_one_layer}
    \begin{aligned}
        \delta^{(l)} &= \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{z}^{(l)}}
        \\
        &= \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{z}^{(l+1)}} \cdot \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \cdot \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}}
        \\
        &= (\mathbf{W}^{(l+1)})^T \delta^{(l+1)} \odot g_{l}'(\mathbf{z}^{(l)}),
    \end{aligned}
\end{equation}

where $\mathbf{z}^{(l+1)} = \mathbf{W}^{(l+1)} \mathbf{a}^{(l)} + \mathbf{b}^{(l+1)}$, $\mathbf{a}^{(l)} = g_{l}(\mathbf{z}^{(l)})$ from the equation \ref{eq:input_output_neuron} are plugged into the second line in \ref{eq:error_in_one_layer} and the operation $\odot$ stands for Hadamard product.

After the above deduction, the equation \ref{eq:Chain_rule_partial} together with \ref{eq:two_terms} and \ref{eq:error_in_one_layer} are rewritten in the following form:

\begin{equation}
    \label{eq:Chain_rule_partial_deducted}
    \begin{aligned}
        \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{W}^{(l)}} &= \delta^{(l)} (\mathbf{a}^{(l-1)})^T \quad \in \mathbb{R}^{M_{l} \times M_{l-1}}
        \\
        \frac{\partial \mathcal{L}\left( y,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{b}^{(l)}} &= \delta^{(l)} \quad \in \mathbb{R}^{M_{l}}.
    \end{aligned}
\end{equation}

The algorithm \ref{alg:BP_alg} presents the gradient descent training procedure using the backpropagation algorithm.

\clearpage
\begin{algorithm}[h!]
\caption{Backpropagation in Stochastic Gradient Descent}
The backpropagation algorithm requires initial parameters $\theta_{0}$, which is computed by the feedforward propagation algorithm \ref{alg:feedforward_alg}. After the neural network model adapts to the initial parameters $\theta_{0}$:
\label{alg:BP_alg}
    \begin{algorithmic}
    \Require neural network $f(\mathbf{x};\mathbf{W},\mathbf{b})$ with $l$th layer
    \Require training set $\mathcal{D}=\{(\mathbf{x}_{n}, \mathbf{y}_{n})\}_{N}^{n=1}$
    \Require weight matrix $\mathbf{W} \in \mathbb{R}^{M_{l} \times M_{l-1}}$, bias matrix $\mathbf{b} \in \mathbb{R}^{M_{l}}$, initial parameters $\theta_{0}$
    \Require activation function $g(z)$
    \Require regularization parameter $\lambda$
    \Ensure $\nabla \mathcal{L}\left( y_{n},f(\mathbf{x};\mathbf{\theta}) \right) = 0$
    \While{$\nabla \mathcal{L}\left( y_{n},f(\mathbf{x};\mathbf{\theta}) \right) \neq 0$}
    \For{$n=1...N$}
        \For{$l=1...L$}
            \State $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$ \Comment{Compute the input of neurons in the $l$th layer}
            \State $\mathbf{a}^{(l)} = g_{l}(\mathbf{z}^{(l)})$ \Comment{Compute the output of neurons in the $l$th layer}
            \State \Comment{See the equation \ref{eq:input_output_neuron}}
            \State $\delta^{(l)} = (\mathbf{W}^{(l+1)})^T \delta^{(l+1)} \odot g_{l}'(\mathbf{z}^{(l)})$ \Comment{Compute the error in the $l$th layer}
            \State \Comment{See the equation \ref{eq:error_in_one_layer}}
            \State $\frac{\partial \mathcal{L}\left( y_n,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T$ \Comment{Compute the gradient on the weight matrix}
            \State $\frac{\partial \mathcal{L}\left( y_n,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}$ \Comment{Compute the gradient on the bias matrix}
            \State \Comment{See the equation \ref{eq:Chain_rule_partial_deducted}}
            \State $\mathbf{W}^{(l+1)} \gets  \mathbf{W}^{(l)}-\alpha (\delta^{(l)} (\mathbf{a}^{(l-1)})^T + \lambda \mathbf{W}^{(l)})$ \Comment{Update the weight matrix}
            \State $\mathbf{b}^{(l+1)} \gets  \mathbf{b}^{(l)}-\alpha \delta^{(l)}$ \Comment{Update the bias matrix}
            \State \Comment{See the equation \ref{eq:GD}}
        \EndFor
    \EndFor
    \EndWhile
    \State \Return $\mathbf{W}, \mathbf{b}$ \Comment{Return the parameters}
    \end{algorithmic}
\end{algorithm}

The regularization parameter $\lambda$ is used to prevent overfitting. The reason for overfitting is that some of the parameters are way too large. Adding a regularization term can restrict the parameters in order to prevent overfitting. Note that the regularization parameter $\lambda$ may also appear in the step of computing the gradient on the weight and bias matrices in some other backpropagation algorithms.

\subsection{Adam Algorithm}
As can be seen from the backpropagation algorithm \ref{alg:BP_alg} above, the gradient descent algorithm is actually applied in the step of updating the parameters. So are there any other algorithms to update the parameters besides the gradient descent algorithm? The answer is yes. For example, one can use the momentum method to replace the gradient $\nabla \mathcal{L}(\cdot)$ with momenta and the RMSProp algorithm (Root Mean Squared Propagation algorithm) \cite{tieleman2012divide} to update the learning rate $\alpha$. An algorithms that combines the ideas of the momentum method and the RMSProp algorithm is the Adam algorithms (Adaptive Moment Estimation Algorithm) \cite{kingma2014adam}. In practice, the Adam algorithm shows better performance, i.e. fewer parameters to tune, resulting in higher computational efficiency.

The Adam algorithm uses mini-batch gradient descent \cite{bottou2010large}. Consider a neural network $f(\mathbf{x};\mathbf{W},\mathbf{b})$, a training set $\mathcal{D}=\{(\mathbf{x}_{n}, \mathbf{y}_{n})\}_{N}^{n=1}$. Mini-batch gradient descent algorithm splits the training set $\mathcal{D}$ into a sequence of subsets (or mini batches) $\mathcal{S}_t=\{(\mathbf{x}_{m}, \mathbf{y}_{m})\}_{M}^{m=1}$ for each iteration $t$, where $M$ is the batch size of one of the mini batches.

Similar to the idea of gradient descent (see \ref{eq:GD}), the point of the Adam algorithm is to iterate the following procedure:

\begin{equation}
    \label{eq:Adam_GD}
    \begin{aligned}
        \theta_{t+1} = \theta_{t} - \alpha \frac{\hat{s}}{\sqrt{\hat{r}} + \epsilon},
    \end{aligned}
\end{equation}

where $\epsilon$ is a very small constant for stabilization (avoiding a denominator of zero). And the two terms $\hat{s}$ and $\hat{r}$ are known as correct moment estimate. It corrected the bias of the original moment estimate $s$ and $r$ at the beginning of the iteration:

\begin{equation}
    \label{eq:correct_moment_estimate}
    \begin{aligned}
        \hat{s_t} &= \frac{s_t}{1-\beta_1^t}
        \\
        \hat{r_t} &= \frac{r_t}{1-\beta_2^t},
    \end{aligned}
\end{equation}

where $\beta_1^t$ and $\beta_2^t$ are learning rate decay. The moment estimate $s_t$ and $r_t$ are defined as following:

\begin{equation}
    \label{eq:moment_estimate}
    \begin{aligned}
        s_t &= \beta_1 s_{t-1} + (1-\beta_1) \mathbf{g_t}
        \\
        r_t &= \beta_2 r_{t-1} + (1-\beta_2) \mathbf{g_t} \odot \mathbf{g_t}
    \end{aligned}
\end{equation}

where $\mathbf{g_t}$ stands for the gradient of the loss function with respect to the parameter at $t$th iteration over the mini-batch:

\begin{equation}
    \label{eq:mean_gradient}
    \mathbf{g_t} = \frac{1}{M} \sum\nolimits_{m=1}^M \nabla \mathcal{L}\left( y_{m},f(\mathbf{x};\mathbf{\theta}) \right).
\end{equation}
 

\clearpage
\begin{algorithm}[h!]
\caption{The training procedure of the Adam algorithm}
The Adam algorithm also requires initial parameters $\theta_{0}$, which is computed by the feedforward propagation algorithm \ref{alg:feedforward_alg}. The decay rates $\beta_1$ and $\beta_2$ should be within the range of $[0,1)$, f.e., $\beta_1=0.9$ and $\beta_2=0.99$. The learning rate $\alpha$ is recommended to be 0.001. And the very small constant $\epsilon$ is set to $10^{-8}$.
\label{alg:Adam_alg}
    \begin{algorithmic}
    \Require neural network $f(\mathbf{x};\mathbf{W},\mathbf{b})$ with $l$th layer
    \Require weight matrix $\mathbf{W} \in \mathbb{R}^{M_{l} \times M_{l-1}}$, bias matrix $\mathbf{b} \in \mathbb{R}^{M_{l}}$, initial parameters $\theta_{0}$
    \Require activation function $g(z)$
    \Require decay rate $\beta_1$ and $\beta_2$
    \Require learning rate $\alpha$
    \Require constant $\epsilon$
    \Ensure $\nabla \mathcal{L}\left( y_{m},f(\mathbf{x};\mathbf{\theta}) \right) = 0$
    \While{$\nabla \mathcal{L}\left( y_{m},f(\mathbf{x};\mathbf{\theta}) \right) \neq 0$}
    \For{$n=1...N$}
        \For{$l=1...L$}
            \State $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$ \Comment{Compute the input of neurons in the $l$th layer}
            \State $\mathbf{a}^{(l)} = g_{l}(\mathbf{z}^{(l)})$ \Comment{Compute the output of neurons in the $l$th layer}
            \State \Comment{See the equation \ref{eq:input_output_neuron}}
            \State $\delta^{(l)} = (\mathbf{W}^{(l+1)})^T \delta^{(l+1)} \odot g_{l}'(\mathbf{z}^{(l)})$ \Comment{Compute the error in the $l$th layer}
            \State \Comment{See the equation \ref{eq:error_in_one_layer}}
            \State $\frac{\partial \mathcal{L}\left( y_m,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T$ \Comment{Compute the gradient on the weight matrix}
            \State $\frac{\partial \mathcal{L}\left( y_m,f(\mathbf{x};\mathbf{\mathbf{W},\mathbf{b}}) \right)}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}$ \Comment{Compute the gradient on the bias matrix}
            \State \Comment{See the equation \ref{eq:Chain_rule_partial_deducted}}
        \EndFor
    \EndFor
    \State $\mathbf{g_t} \gets \frac{1}{M} \sum\nolimits_{m=1}^M \nabla \mathcal{L}\left( y_{m},f(\mathbf{x};\mathbf{\theta}) \right)$ \Comment{Compute the mean gradient, see the equation \ref{eq:mean_gradient}}
    \State $s_t \gets \beta_1 s_{t-1} + (1-\beta_1) \mathbf{g_t}$ \Comment{Compute the fist moment estimate}
    \State $r_t \gets \beta_2 r_{t-1} + (1-\beta_2) \mathbf{g_t} \odot \mathbf{g_t}$ \Comment{Compute the second moment estimate}
    \State \Comment{See the equation \ref{eq:moment_estimate}}
    \State $\hat{s_t} \gets \frac{s_t}{1-\beta_1^t}$ \Comment{Correct the bias}
    \State $\hat{r_t} \gets \frac{r_t}{1-\beta_2^t}$ \Comment{Correct the bias}
    \State \Comment{See the equation \ref{eq:correct_moment_estimate}}
    \State $\theta_{t+1} \gets \theta_{t} - \alpha \frac{\hat{s}}{\sqrt{\hat{r}} + \epsilon}$ \Comment{Update the parameters}
    \State \Comment{See the equation \ref{eq:Adam_GD}}
    \EndWhile
    \State \Return $\mathbf{W}, \mathbf{b}$ \Comment{Return the parameters}
    \end{algorithmic}
\end{algorithm}


\clearpage
\chapter{Neural ODE}
In order to obtain more accurate results from a neural network model, one may tend to think of stacking more hidden layers. However, when some data scientists try to build complex models using neural networks with hundreds of hidden layers, they found that the vanishing or exploding gradient problem often occurs in backpropagation, which seriously affects the learning efficiency of the upstream hidden layers \cite{glorot2010understanding}. Due to the vanishing or exploding gradient problem, a deeper neural network could even yield worse results than a shallower one. And ResNet (Residual Neural Network) was exactly designed to tackle these problems \cite{he2016deep}. The proposers of ResNet argue that the results of deeper neural networks can be better or at least equal to the results of shallower neural networks, but should not be worse.

\section{Residual Neural Network}
The ResNet breaks the traditional convention that the output of $l$th layers of a neural network can only be given to $l+1$th layers as input. The ResNet allows the output of one layer $\mathbf{x}$ can directly skip several weight layers $F(\mathbf{x};\theta)$ and then used as the input to a later layer:

\begin{equation}
    \label{eq:ResNet}
    H(\mathbf{x})=\mathbf{x}+F(\mathbf{x};\theta),
\end{equation}

where $H$ is the output or target function. The residual building block illustrates what equation \ref{eq:ResNet} does:

\clearpage
\begin{figure}[htbp!]
    \centering
    \includegraphics[scale=1]{figures/ResNet.pdf}
    \caption{Residual building block}
    \label{fig:ResNet}
\end{figure}

The weight layers could be thought of as two or more hidden layers that is squeezed up and then skipped over by a shortcut connection. This shortcut connection changes the learning target from learning the complete output $H(\mathbf{x})$ to the residual $H(\mathbf{x})-\mathbf{x}$. And the goal of the optimization is to minimize the residual $H(\mathbf{x})-\mathbf{x}$ and make it close to zero. In this case, when $F(\mathbf{x};\theta)=H(\mathbf{x})-\mathbf{x}=0$, then $H(\mathbf{x})=\mathbf{x}$. Since the output $H(\mathbf{x})$ and the input $\mathbf{x}$ are identical, $\mathbf{x}$ is also called identity. The proposers of ResNet hypothesized that the residual is easier to optimize than the complete output and then verified this hypothesis in their experiments. 

 The significance of ResNet is that it provides a new direction to address the challenges of stacking more and more hidden layers to the neural networks. Benefiting from the ResNet, the vanishing or exploding gradient problem can be solved to some extent.

\section{Neural ODE}
The hidden layers in a ResNet are built by a sequence of transformations in equation \ref{eq:ResNet}. This sequence of transformations was found to be remarkably similar to the Euler method \ref{eq:Eulers_method} \cite{ruthotto2020deep}. In view of this, equation \ref{eq:ResNet} can also be recasted into:

\begin{equation}
    \label{eq:Euler_discretization}
    \mathbf{z}_{t+1} = \mathbf{z}_{t} + f(\mathbf{z}_{t}; \theta),
\end{equation}

where $t \in {0 ... T}$ and $\mathbf{z}_{t}$ are some Euler discretizations of a continuous transformation.

Equation \ref{eq:Euler_discretization} can be rewritten in the form:

\begin{equation}
    \label{eq:Euler_discretization_rewritten}
    \frac{\mathbf{z}_{t+1}-\mathbf{z}_{t}}{\Delta t} = f(\mathbf{z}_{t}; \theta),
\end{equation}

where $\Delta t = (t+1)-t = 1$. If $\Delta t$ is a very small step, and then, a Euler discretization

\begin{equation}
    \label{eq:Euler_discretization_small_step}
    \lim_{\Delta t \to 0} \frac{\mathbf{z}_{t+1}-\mathbf{z}_{t}}{\Delta t} = f(\mathbf{z}_{t}; \theta)
\end{equation}

can be seen as the gradient of $z(t)$ by the definition of differentiation, i.e.

\begin{equation}
    \label{eq:Euler_discretization_differentiation}
    \frac{d\mathbf{z}(t)}{dt} = f(\mathbf{z}(t), t; \theta).
\end{equation}

The above equation \ref{eq:Euler_discretization_differentiation} is called Neural ODE (Neural Ordinary Differential Equation). Different from the explicit ODE in the form of equation \ref{eq:explicit_ODE} that can be solved by using traditional numerical methods such as Euler method, the Neural ODE is solved by using an special ODE solver that treated as a black box \cite{chen2018neural}. The RHS $f(\mathbf{z}(t), t; \theta)$ can be considered as a neural network $NN(\mathbf{z}(t), t; \theta)$. Thus, the loss function to be optimized looks like:

\begin{equation}
    \label{eq:Neural_ODE_Gradient_loss}
    \begin{aligned}
    \mathcal{L}(\mathbf{z}(t+\varepsilon )) &= \mathcal{L}(\mathbf{z}(t) + \int_{t}^{t+\varepsilon } f(\mathbf{z}(t), t, \theta)dt \:)\\
    &= \mathcal{L}(ODESolver(\mathbf{z}(t), f, t, t+\varepsilon , \theta)),
    \end{aligned}
\end{equation}

where $\varepsilon $ is a very small time step.

To optimize the loss function \ref{eq:Neural_ODE_Gradient_loss} in backpropagation, one need to compute the gradient of the loss function as the form of equation \ref{eq:Gradient_loss}. The next step, to calculate the error, is to mimic the procedure in equation \ref{eq:error_in_one_layer}:

\begin{equation}
    \label{eq:Neural_ODE_error}
    \frac{d\mathcal{L}}{d\mathbf{z}(t)} = \frac{d\mathcal{L}}{d\mathbf{z}(t+\varepsilon )} \cdot \frac{d\mathbf{z}(t+\varepsilon )}{d\mathbf{z}(t)}.
\end{equation}

\section{Adjoint Method}
The adjoint method is a method used to compute the gradients of functions efficiently, which can be dated back to the 1960s \cite{boltyanskiy1962mathematical}. Due to the continuity of Neural ODE, computing the gradient of the loss function in backpropagation leads to a huge memory cost. The significance of using the adjoint method is that the gradient of the loss function can still be computed efficiently without storing the intermediate activations which cost a huge memory overhead.

Similar to the stochastic gradient descent algorithm \ref{alg:BP_alg} in the preceding chapter, the goal of the adjoint method is also to compute the gradient of the loss function with respect to the state $\frac{d\mathcal{L}}{d\mathbf{z}(t)}$ and with respect to the parameters $\frac{d\mathcal{L}}{d\theta}$.

To use the adjoint method, the first step is to define an adjoint state that equal to the gradient of the loss function with respect to the state:

\begin{equation}
    \label{eq:adjoint_state}
    \mathbf{a}(t) = \frac{d\mathcal{L}}{d\mathbf{z}(t)}.
\end{equation}

After that, expand the Taylor series for the hidden state $\mathbf{z}(t+\varepsilon )$ at the point $\mathbf{z}(t)$:

\begin{equation}
    \label{eq:taylor_series_expansion}
    \begin{aligned}
    \mathbf{z}(t+\varepsilon ) &= \mathbf{z}(t) + \int_{t}^{t+\varepsilon } f(\mathbf{z}(t), t, \theta)dt\\
    &= \mathbf{z}(t) + \varepsilon f(\mathbf{z}(t), t; \theta) + O (\varepsilon ^2).
    \end{aligned}
\end{equation}

Then plug the equations \ref{eq:adjoint_state} and \ref{eq:taylor_series_expansion} into the equation \ref{eq:Neural_ODE_error}:

\begin{equation}
    \label{eq:Neural_ODE_adjoint_state}
    \mathbf{a}(t) = \mathbf{a}(t+\varepsilon ) \cdot \frac{\partial}{\partial \mathbf{z}(t)} (\mathbf{z}(t) + \varepsilon f(\mathbf{z}(t), t; \theta) + O (\varepsilon ^2)) .
\end{equation}

By the definition of differentiation, the gradient of the adjoint state follows:

\begin{equation}
    \label{eq:adjoint_state_gradient_hidden_state}
    \begin{aligned}
        \frac{d\mathbf{a}(t)}{dt} &= \lim_{\varepsilon \to 0+} \frac{\mathbf{a(t+\varepsilon)}-\mathbf{a(t)}}{\varepsilon} \\
        &= \lim_{\varepsilon \to 0+} \frac{\mathbf{a(t+\varepsilon)}-\mathbf{a}(t+\varepsilon ) \cdot \frac{\partial}{\partial \mathbf{z}(t)} (\mathbf{z}(t) + \varepsilon f(\mathbf{z}(t), t; \theta) + O (\varepsilon ^2)) }{\varepsilon}\\
        &= \lim_{\varepsilon \to 0+} \frac{-\varepsilon \mathbf{a(t+\varepsilon)} \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)} + O (\varepsilon ^2)}{\varepsilon}\\
        &= \lim_{\varepsilon \to 0+} - \mathbf{a(t+\varepsilon)} \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)} + O (\varepsilon)\\
        &= - \mathbf{a(t)} \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)}
    \end{aligned}
\end{equation}

Now perform the feedforward propagation algorithm \ref{alg:feedforward_alg}  to get the initial output and compute the initial adjoint state $\mathbf{a}(T) = \frac{d\mathcal{L}}{d\mathbf{z}(T)}$ with reference to the equation \ref{eq:adjoint_state}. This can be considered as the initial condition of an ODE problem. And the corresponding ODE is of the form:

\begin{equation}
    \label{eq:adjoint_state_ODE}
    \begin{aligned}
    \mathbf{a}(T-\varepsilon) &= \mathbf{a}(T) + \int_{T}^{T-\varepsilon} \frac{d\mathbf{a(t)}}{dt} \\
    &= \mathbf{a}(T) - \int_{T}^{T-\varepsilon} \mathbf{a(t)}^T \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)} \quad \text{plugging} \: \ref{eq:adjoint_state_gradient_hidden_state} \\ 
    \end{aligned}
\end{equation}

Note that the state adjoint $\mathbf{a(t)}^T$ is transposed due to the "backward" direction.

Similar to the equation \ref{eq:adjoint_state} $\mathbf{a}(t) = \frac{d\mathcal{L}}{d\mathbf{z}(t)}$, the gradient of the loss function with respect to the parameters is defined by:

\begin{equation}
    \label{eq:adjoint_state_parameters}
    \mathbf{a_\theta}(t) = \frac{d\mathcal{L}}{d\theta}.
\end{equation}

The corresponding gradient of the adjoint state is of the form:

\begin{equation}
    \label{eq:adjoint_state_gradient_parameters}
    \frac{d\mathcal{L}}{d\theta} = - \int_{T}^{0} \mathbf{a(t)}^T \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{\theta}} dt
\end{equation}

The detailed deduction procedures for $\frac{d\mathcal{L}}{d\theta}$ can be found in the original paper \cite{chen2018neural}.

A short summary: From the above procedures, it can be concluded that the idea of the adjoint method in Neural ODE is to compute a sequence of adjoint states in a "backward" direction by solving ODEs, and therefore the gradients can also be computed in an indirect manner.

\section{Dynamic Simulation With Neural ODE}
\subsection{Undamped Harmonic Oscillator}
\subsection{Isothermal Damped Harmonic Oscillator}


\clearpage
\chapter{Structured Neural ODE}
\section{Structure}
\section{Dynamic Simulation with Structured Neural ODE}

\clearpage
\chapter{Parameter Estimation}


\clearpage
\chapter{Conclusion}
\section{Summary}
\section{Outlook}


%%%%%%%%%%%%%%
% REFERENCES %
%%%%%%%%%%%%%%
\bibliographystyle{unsrt}
\renewcommand{\bibname}{References} % Title References instead of Bibliography
\bibliography{literature/literature}


%%%%%%%%%%%%
% APPENDIX %
%%%%%%%%%%%%
\appendix


\end{document}