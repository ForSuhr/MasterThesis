%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT CLASS AND GENERAL OPTIONS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[
	parskip, 			   % german paragraph style (no indentation, blank line)
	twoside, 			   % two-sided document
	DIV=14, 			   % narrow margin
	BCOR=15.0mm, 		   % binding correction
	headsepline, 		   % header line
	open=right, 		   % new chapters start on right hand side
	captions=tableheading, % correct distance between table and caption above
	bibliography=totoc,    % references shown in contents
	numbers=noenddot       % remove dots at the the end of chapter numbers
]{scrreprt}

\newcommand{\languages}{english}

\makeatletter
\newcommand\addcase[3]{\expandafter\def\csname\string#1@case@#2\endcsname{#3}}
\newcommand\makeswitch[2][]{%
	\newcommand#2[1]{%
		\ifcsname\string#2@case@##1\endcsname\csname\string#2@case@##1\endcsname\else#1\fi%
	}%
}
\makeatother

\makeswitch[nada]\dothis
\addcase\dothis{english}{\include{prechapters/declaration}}
\addcase\dothis{ngerman}{\include{prechapters/Eigenstaendigkeitserklaerung}}


%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGES AND SETTINGS %
%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} % input umlaut characters directly
\usepackage[T1]{fontenc} % proper output of umlaut characters
\usepackage[ngerman, \languages]{babel} % hyphenation, main language in the end
\usepackage{scrlayer-scrpage} % customise head and foot
\automark[chapter]{chapter} % display chapter name on all pages instead of section name on right hand side
\usepackage{color} % definition of own colors
\definecolor{LTD_grey}{gray}{0.65} % LTD Grau nach dem neuen, druckkostensparenden caltechgray
\addtokomafont{pagehead}{\color{LTD_grey} \sffamily \upshape} % head in grey and sans serif
\addtokomafont{pagenumber}{\color{LTD_grey}} % page number in grey
\addtokomafont{subject}{\normalfont} % Thesis type not bold
\usepackage{graphicx} % scaling of graphics
\usepackage{booktabs} % lines for tabs
\usepackage{amssymb,amsmath} % math symbols
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{newunicodechar}
\newunicodechar{θ}{$\theta$}
\newunicodechar{α}{$\alpha$}
\newunicodechar{β}{$\beta$}
\newunicodechar{∂}{$\partial$}
\newunicodechar{Δ}{$\Delta$}


%%%%%%%%%%%
% CONTENT %
%%%%%%%%%%%
\begin{document}
\pagenumbering{roman} % roman numbering before main part


%%%%%%%%%%%%%%%%%%%%
% TITLE AND AUTHOR %
%%%%%%%%%%%%%%%%%%%%
\title{On the identification of port-Hamiltonian models via machine-learning} % title
\subject{Master's thesis} % type (Bachelor's thesis / Master's thesis / project thesis)
\author{Jiandong Zhao} % author
 \date{January 2023} % period
 \publishers{supervised by\\[1em]
	Prof. Dr.-Ing. habil. S. Leyendecker\\
	M. Sc. Markus Lohmayer % name of supervisor
	}
\titlehead{\includegraphics[height=1.45cm]{figures/uni_head_new} \hfill \includegraphics[height=1.3cm]{figures/ltd_head}}

% title page
\maketitle


%%%%%%%%%%%%%%%%%%%%%%
% CONTENTS AND LISTS %
%%%%%%%%%%%%%%%%%%%%%%
%% declaration
\dothis{ngerman}

% abstract
\include{prechapters/abstract}

% contents
\tableofcontents

% figure list
%\listoffigures

% table list
%\listoftables

\cleardoublepage % finish current page with roman numbering


%%%%%%%%%%%%%%%%
% MAIN CONTENT %
%%%%%%%%%%%%%%%%
\pagenumbering{arabic} % arabic numbering for main part


% chapters
\chapter{Introduction}
\label{ch:chapter1}
\section{Motivation}
Today, neural networks are widely used in various fields. For Hamiltonian mechanics, a paper \cite{greydanus2019hamiltonian} proposed Hamiltonian Neural Networks. They encode underlying physical laws as prior knowledge, such that a Hamiltonian Neural Network can learn a conserved quantity that is analogous to the total energy of a Hamiltonian system. A following paper \cite{zhong2020dissipative} introduced Dissipative SymODEN, which is a deep learning architecture designed to learn dynamics of port-Hamiltonian systems. Both the above models are based on a state of the art technique called Neural ODEs \cite{chen2018neural}. A Neural ODE is an ODE like $\dot{x} = f_{\theta}(x, t)$, which has a neural network on its right hand side (RHS). In a Neural ODE, its RHS is unknown and the neural network $f_{\theta}$ is treated as a black box model for the whole system. By integrating the Neural ODE with numerical methods, we can obtain the system dynamics.

Port-Hamiltonian systems theory provides a port-based approach to modelling, optimization and control of multiphysical systems. Exergetic Port-Hamiltonian Systems (EPHS) proposed in \cite{lohmayer2021exergetic} are port-Hamiltonian systems endowed with thermodynamic structure. In contrast to traditional port-Hamiltonian systems, EPHS further provides an explicit relation between port-Hamiltonian systems and thermodynamics. In the EPHS modelling framework, an EPHS model is considered as a model composed by system components, where some components may be unknown. To build a model without knowning all components, techniques like Neural ODEs provide a new direction. However, in the case where a part of system components are known, it seems unnecessary to treat the whole system as a black box. Can we replace only the unknown components with neural network models? Can these neural network models be reused in other systems?

\section{Main Contributions}
The main contributions of this thesis is two-fold. We first explain structured Neural ODEs, which are Neural ODEs endowed with structure composing the known parts and unknown parts. 

Later, we propose two approaches for EPHS modelling, where structured Neural ODEs are used. To evaluate our models, we compare the prediction with the ground truth and reuse the neural network models for other systems.

\section{Outline}
An outline for the following chapters:

\textbf{Chapter} \ref{ch:chapter2} provides a quick overview of ordinary differential equations and initial value problems. It also recaps some numerical methods, in particular a symplectic integrator that will be used in later chapters.

\textbf{Chapter} \ref{ch:chapter3} overviews Hamiltonian systems and (exergetic) port-Hamiltonian systems, since this thesis will focus on modelling physical systems as (exergetic) port-Hamiltonian systems.

\textbf{Chapter} \ref{ch:chapter4} gives an introduction to neural networks, which is fundamental for the understanding of later chapters.

\textbf{Chapter} \ref{ch:chapter5} reviews the idea of Neural ODEs and the adjoint method.

\textbf{Chapter} \ref{ch:chapter6} first explores physics priors. Then, it overviews different neural networks based on Neural ODEs: O-NETs, H-NETs and HNNs. Finally, structured ODE neural networks are explained and compared with the other neural network models in code and experiments.

\textbf{Chapter} \ref{ch:chapter7} proposes two approaches for modelling (exergetic) port-Hamiltonian systems with structured Neural ODEs and performs experiments.

\textbf{Chapter} \ref{ch:chapter8} draws conclusions and provides an outlook for future work.


\clearpage
\chapter{Differential Equations}
\label{ch:chapter2}
For better understanding of Neural ODEs and structured Neural ODEs in later chapters, we first review the definition of ordinary differential equation (ODE). In this thesis, we restrict our study to autonomous systems. For more details about ODEs, we refer to \cite{teschl2012ordinary}.
\section{Ordinary Differential Equations}
Consider a real-valued function $x$ with $k$ continuous derivatives: $x \in C^{k}(I)$, where the time interval $ I \subseteq \mathbb{R}, \: k \in \mathbb{N}, \: x: \mathbb{R} \rightarrow \mathbb{R}$. An implicit ODE is an equation of the form:

\begin{equation}
    \label{eq:implicit_ODE}
    F\left(x, x^{(1)}, \ldots, x^{(k)}\right)=0.
\end{equation}

We assume that the highest order derivative $x^{(k)}$ is solvable, and placing it on the left hand side (LHS) alone, it becomes the explicit ordinary differential equation of the form:

\begin{equation}
    \label{eq:explicit_ODE}
    x^{(k)} = F^{\prime} \left(x, x^{(1)}, \ldots, x^{(k-1)}\right).
\end{equation}

More general, consider the case $x: I \rightarrow \mathbb{R}^{n}$. Equation \ref{eq:explicit_ODE} can be extended to a system of ordinary differential equations:

\begin{equation}
    \label{eq:ODE_system}
    \begin{aligned}
    x_{1}^{(k)} &=F_{1}\left(x, x^{(1)}, \ldots, x^{(k-1)}\right) \\
    x_{2}^{(k)} &=F_{2}\left(x, x^{(1)}, \ldots, x^{(k-1)}\right) \\
    & \vdots \\
    x_{n}^{(k)} &=F_{n}\left(x, x^{(1)}, \ldots, x^{(k-1)}\right).
\end{aligned}
\end{equation}

The above form is a N-dimensional system, which has $N$ ordinary differential equations.

\section{Initial Value Problems}
An initial value problem (IVP) consists of an explicit ODE (or a system of ODEs) and an initial state:

\begin{equation}
    \label{eq:IVP}
    \dot{x} = f(t, x), \quad x(t_{0})=x_{0}.
\end{equation}

By integrating the explicit ODE, we obtain an integral equation of the form,

\begin{equation}
    \label{eq:solution_IVP}
    x(t) = x_{0} + \int_{t_{0}}^{t} f(s, x(s))\:ds,
\end{equation}

which is an analytical solution.


\clearpage
\section{Numerical Methods}
In this section, we discuss some numerical methods for solving IVPs. We refer to the solutions provided by numerical methods as numerical solutions. In addition, the algorithmic descriptions regarding numerical methods are known as numerical schemes. For details about numerical methods, we refer to \cite{hairer2006geometric}.

Suppose that the time evolution in \ref{eq:solution_IVP} ends with $T$, we can obtain a sequence of approximating solutions $ \{ x_{t_{i}} \}_{t_{0}}^{T}$. In dynamical systems, $ \{ x_{t_{i}} \}_{t_{0}}^{T}$ is a sequence of points in state space, which is also known as state trajectory. Although numerical methods provide approximations rather than exact solutions, they are widely implemented since they are easier to compute and more efficient in computer programs. 

\subsection{Explicit Euler Method}
The Euler method is the simplest and probably the first numerical method formulated by Leonhard Euler. Consider an IVP $\dot{x} = f(x), \quad x(t_{0})=x_{0}$. The explicit Euler method is of the form

\begin{equation}
    \label{eq:Eulers_method_explicit}
    x_{n+1} = x_{n} + h \cdot f_{n},
\end{equation}

where h is the time step size $h = t_{n+1} - t_{n}$ and $f_{n}$ is the time derivative of $x$ at time $t_{n}$, i.e. $f_{n} = f(x_n) = \left. \frac{dx}{dt} \right|_{t=t_n}$.

\subsection{Implicit Euler Method}
The implicit Euler method is of the form

\begin{equation}
    \label{eq:Eulers_method_implicit}
    x_{n+1} = x_{n} + h \cdot f_{n+1},
\end{equation}

where $f_{n+1} = f(x_{n+1}) = \left. \frac{dx}{dt} \right|_{t=t_{n+1}}$. Comparing to the explicit method \ref{eq:Eulers_method_explicit}, the solution $x_{n+1}$ is defined implicitly. Hence, to obtain the solution of an IVP by using such an implicit method, we need to solve nonlinear equations, given that $f$ is nonlinear.

\subsection{Implicit Midpoint Rule}
The implicit midpoint is of the form

\begin{equation}
    \label{eq:Midpoint_rule_implicit}
    x_{n+1} = x_{n} + h \cdot f\left(\frac{x_{n} + x_{n+1}}{2}\right).
\end{equation}

The implicit midpoint rule is a symplectic integrator. In contrast to \ref{eq:Eulers_method_explicit} and \ref{eq:Eulers_method_implicit}, this symplectic integrator allows the solution trajectory to remain unchanged after inverting the direction. This property is known as symmetry \cite{hairer2006geometric}. In more detail, after exchanging $x_{n+1}$ and $x_{n}$, Equation \ref{eq:Midpoint_rule_implicit} can be rewritten as

\begin{equation}
    \label{eq:Midpoint_rule_implicit_inverted}
    x_{n} = x_{n+1} + h \cdot f\left(\frac{x_{n} + x_{n+1}}{2}\right).
\end{equation}

Such an inversion only changes the direction of the solution trajectory but does not affect the solution trajectory itself. This property of the symplectic integrator makes it useful for reversible systems, such as Hamiltonian systems.

\clearpage
\chapter{Port-Hamiltonian Systems}
\label{ch:chapter3}

\section{Hamiltonian Systems}

\subsection{Formulation}
In general, a Hamiltonian system is a triple $(\mathcal{X},\omega,H)$, where $(\mathcal{X},\omega)$ is a symplectic manifold which consists of a manifold $\mathcal{X}$ and a symplectic structure (or symplectic form) $\omega$. The Hamiltonian or Hamiltonian function $H: \mathcal{X} \mapsto \mathbb{R} $ is a smooth function on the manifold $\mathcal{X}$, i.e., $H \in C^{\infty}(\mathcal{X})$ \cite{rudolph2017differential}. And the Hamiltonian vector field corresponding to the Hamiltonian function $H$ is denoted by $X_H = \{H, \cdot \}$, where $\{\cdot, \cdot\}$ is a Poission bracket. Suppose that the Poission bracket is on the symplectic manifold $(\mathcal{X},\omega)$ and a smooth function $f$ is on the manifold $\mathcal{X}$, i.e., $f \in C^{\infty}(\mathcal{X}) $. Thus, the evolution of $f$ can be given by $\dot{f} = \{H, f\} = X_H(f)$. For more details, we refer to \cite{rudolph2012differential}.

In mechanical systems, the Hamiltonian can be formulated by $H(\mathbf{q},\mathbf{p})=T(\mathbf{q},\mathbf{p})+V(\mathbf{q})$, where $T(\mathbf{q},\mathbf{p})$ is the kinetic energy and $V(\mathbf{q})$ is the potential energy of the system. With the generalized coordinate $\mathbf{q} = (q^1, q^2, ..., q^n)$ and generalized momentum $\mathbf{p} = (p_1, p_2, ..., p_n)$, the Hamiltonian is of the form

\begin{equation}
    \label{eq:Hamiltonian}
    H(\mathbf{q},\mathbf{p})=\frac{1}{2}\mathbf{p}^T\mathbf{M}^{-1}(\mathbf{q})\mathbf{p} + \mathbf{V}(\mathbf{q}),
\end{equation}

where $\mathbf{M}(\mathbf{q})$ is the mass matrix, which expresses the inertia of the system.

\subsection{Dynamics}
We use canonical coordinates $q^i$ and $p_i$, which are sets of coordinates in phase space, to describe the Hamiltonian systems. Consider $q^i$ and $p_i$ are smooth functions on the manifold $\mathcal{X}$, i.e., $q^i \in C^{\infty}(\mathcal{X})$ and $p_i \in C^{\infty}(\mathcal{X})$. Suppose that the Poission bracket $\{H, f\}$ is on the symplectic manifold $(\mathcal{X},\omega)$. Hence, the Hamiltonian vector field can be written as 

\begin{equation}
    \label{eq:Hamiltonian_vector_field}
    X_H = \sum\nolimits_{i=1}^n \frac{\partial H}{\partial p_i} \frac{\partial}{\partial q^i} - \frac{\partial H}{\partial q^i} \frac{\partial}{\partial p_i}.
\end{equation}

We also refer to the Hamiltonian vector field $X_H$ as the symplectic gradient of $H$.

Then, the dynamcis of the Hamiltonian systems is given by 

\begin{equation}
    \label{eq:Hamiltonian_dynamcis}
    \begin{aligned}
    \dot{q}^i &= \{H, q^i\} = X_H(q^i)=\frac{\partial H}{\partial p_i},\\
    \dot{p}_i &= \{H, p_i\}= X_H(p_i)=-\frac{\partial H}{\partial q^i}.
    \end{aligned}
\end{equation}


\subsection{Conservation of Energy}
Let the state variables $\mathbf{x}=(\mathbf{q},\mathbf{p}) \in \mathcal{X}$. As the Hamiltonian is time-independent, it holds

\begin{equation}
    \label{eq:Hamiltonian_invariant}
    \frac{dH}{dt} = \left(\frac{\partial H}{\partial \mathbf{x}}\right)^T \frac{d\mathbf{x}}{dt} = 0,
\end{equation}

where the time derivatives of the state variables can be written as

\begin{equation}
    \label{eq:Hamiltonian_symplectic}
    \begin{aligned}
        \frac{d\mathbf{x}}{dt} &= J \frac{\partial H}{\partial \mathbf{x}},\\\\
        \text{where}\\
        J &= \left[ \begin{array}{cc}
            0 & -I \\
            I & 0
        \end{array} \right].
    \end{aligned}
\end{equation}

A matrix $J$ which satisfies $-J = J^T$ is called a skew-symmetric matrix.

Plugging \ref{eq:Hamiltonian_symplectic} into \ref{eq:Hamiltonian_invariant}, we can prove that the Hamiltonian is conserved:

\begin{equation}
    \label{eq:Hamiltonian_symplectic_conclusion}
    \begin{aligned}
        \frac{dH}{dt} &= \left(\frac{\partial H}{\partial \mathbf{x}}\right)^T \frac{d\mathbf{x}}{dt}\\
         &= \left(\frac{\partial H}{\partial \mathbf{x}}\right)^T J \frac{\partial H}{\partial \mathbf{x}} \\
         &= 0.
    \end{aligned}
\end{equation}

\subsection{Symplectic Integration of Hamiltonian Systems}
Applying the midpoint rule to Equation \ref{eq:Hamiltonian_symplectic}, we obtain:

\begin{equation}
    \label{eq:Midpoint_rule_implicit_Hamiltonian}
    x_{n+1} = x_{n} + h \cdot J \nabla H \left(\frac{x_{n} + x_{n+1}}{2}\right).
\end{equation}

The differentiation of $x_{n+1}$ with respect to $x_{n}$ yields

\begin{equation}
    \label{eq:Midpoint_rule_implicit_Hamiltonian_differentiation}
    \left(I - \frac{1}{2} h \cdot J \nabla^2 H\left(\frac{x_{n} + x_{n+1}}{2}\right) \right) \frac{\partial x_{n+1}}{\partial x_{n}} = I + \frac{1}{2} h \cdot J \nabla^2 H\left(\frac{x_{n} + x_{n+1}}{2}\right).
\end{equation}

We reformulate \ref{eq:Midpoint_rule_implicit_Hamiltonian_differentiation} as

\begin{equation}
    \frac{\partial x_{n+1}}{\partial x_{n}} = \frac{I - \frac{1}{2} h \cdot J \nabla^2 H\left(\frac{x_{n} + x_{n+1}}{2}\right) }{I + \frac{1}{2} h \cdot J \nabla^2 H\left(\frac{x_{n} + x_{n+1}}{2}\right)} .
\end{equation}

Then, we can prove the relation

\begin{equation}
    \label{eq:Midpoint_rule_implicit_symplecticity}
    \left(\frac{\partial x_{n+1}}{\partial x_{n}}\right)^T J \left(\frac{\partial x_{n+1}}{\partial x_{n}}\right) = J
\end{equation}

satisfy for all $n$. By definition of symplectic, a linear mapping $A: \mathbb{R}^{2n} \mapsto \mathbb{R}^{2n}$ is called symplectic if $A^T J A = J$ \cite{hairer2006geometric}. Hence, we say $x_{n+1}$ is a symplectic transformation and \ref{eq:Midpoint_rule_implicit_Hamiltonian} is a symplectic integrator.


\subsection{Example: Undamped Harmonic Oscillator}
An undamped harmonic oscillator is a classical Hamiltonian system. A mass $m$ is connected to one end of a spring with compliance $c$, while the other end is fixed. The direction of the displacement $q$ is positive in the direction of being stretched by the mass.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.3]{figures/undamped harmonic oscillator.jpg}
    \caption{Undamped harmonic oscillator.}
    \label{fig:physical_model_undamped_harmonic_oscillator}
\end{figure}

In Newtonian mechanics, according to Newton's second law $F=ma=m\ddot{x}$ and Hooke's law $F_s=kx$ ($F_s$ is the restoring force of a spring, which is always in the opposite direction to the displacement and $k$ is the spring coefficient $k=\frac{1}{c}$), it holds $m\ddot{x}+kx=0$. This implicit ODE describes the system dynamics. As the highest order in the ODE is two, it is called a second order differential equation.

In Hamiltonian mechanics, referring to Equation \ref{eq:Hamiltonian}, the Hamiltonian is the sum of the kinetic energy $\frac{1}{2}\mathbf{p}^T\mathbf{M}^{-1}(\mathbf{q})\mathbf{p}$ and the potential energy $\mathbf{V}(\mathbf{q})$. In the case of undamped harmonic oscillator, the Hamiltonian function can be written as

\begin{equation}
    \label{eq:Hamiltonian_udho}
    H(q,p)=\frac{1}{2c}q^2+\frac{1}{2m}p^2,
\end{equation}

where p is the momentum of the mass. The state of the system (in canonical coordinates) $x=(q,p)$ moves along the Hamiltonian vector field $X_{H}=\{H, \cdot\}$, where the Poission bracket is on the symplectic manifold $(\mathcal{X},\omega)$. According to \ref{eq:Hamiltonian_vector_field}, the evolution of $q$ and $p$ is determined by

\begin{equation}
    \label{eq:ODE_undamped_harmonic_oscillator}
    \begin{aligned}
        \dot{q}&=X_{H}(q)=\frac{\partial H}{\partial p}=\frac{p}{m},\\
        \dot{p}&=X_{H}(p)=-\frac{\partial H}{\partial q}=-\frac{q}{c}.
    \end{aligned}
\end{equation}

Moving along the Hamiltonian vector field keeps the total energy of the system constant. Therefor, the time derivative of the Hamiltonian stays at zero:

\begin{equation}
    \label{eq:derivative_Hamiltonian}
    \dot{H}=\frac{\partial H}{\partial q}\dot{q}+\frac{\partial H}{\partial p}\dot{p}=0.
\end{equation}


\section{Port-Hamiltonian Systems}
The port-Hamiltonian systems formulation provides a port-based modelling approach, where a complex system can be expressed by an interconnection of several components. For more details, we refer to \cite{van2014port}.

\subsection{Dirac Structure}
The Dirac structure is a subbundle

\begin{equation}
    \label{eq:Dirac_structure}
    \forall x \in \mathcal{X}: \mathcal{D}_x \subset T_{x}\mathcal{X} \times T_{x}^{*}\mathcal{X} \times \mathcal{F}_R \times \mathcal{E}_R \times \mathcal{F}_P \times \mathcal{E}_P,
\end{equation}

where a pair $(f_S, e_S) \in T_{x}\mathcal{X} \times T_{x}^{*}\mathcal{X}$ is an energy-storing port, a pair $(f_R, e_R) \in \mathcal{F}_R \times \mathcal{E}_R$ is an energy-dissipating port and a pair $(f_P, e_P) \in \mathcal{F}_P \times \mathcal{E}_P$ is an external port. We call $f_S, f_R, f_P$ the flow variables and $e_S, e_R, e_P$ the effort variables. Dirac structure is a structure that constrains effort and flow variables such that the energy is conserved.

As can be seen in \ref{eq:Dirac_structure}, a Dirac structure is a subbundle $\mathcal{D} \subset \mathcal{F} \times \mathcal{E}$, where $\mathcal{F}$ and $\mathcal{E}$ are the spaces of the port variables (i.e. flows $f$ and efforts $e$). We refer to a pair $(f,e)$ of flow and effort variables as a port. In port-Hamiltonian systems, the subsystems interact with each other via ports. Normally, such interactions are assumed to be the exchanges of energy.

\subsection{EPHS framework}
The EPHS (exergetic port-Hamiltonian systems) framework combines the port-Hamilton systems theory with the GENERIC framework \cite{lohmayer2021exergetic}. In this thesis, we adopt a bond-graph expression proposed by \cite{lohmayer2022ephs}, which is inspired by bond-graph \cite{paynter1961analysis}, to provide a graphical representation of syntactic expression for EPHS. We will follow the EPHS framework in the following example.

\subsection{Example: Isothermal Damped Harmonic Oscillator}
Unlike the previous example, the mechanical energy of an isothermal damped harmonic oscillator dissipates with time.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.3]{figures/isothermal damped harmonic oscillator.jpg}
    \caption{Isothermal damped harmonic oscillator.}
    \label{fig:idho}
\end{figure}

Figure \ref{fig:idho} illustrates an isothermal damped harmonic oscillator, where $d$ is the damping coefficient.

In bond-graph expression, to distinguish different types of subsystems visually, storage components are shown in blue, Dirac structures in green and resistive structures in red:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/bondgraph_idho_exergetic.pdf}
    \caption{Bond-graph expression for isothermal damped harmonic oscillator.}
    \label{fig:bondgraph_idho}
\end{figure}

All components (boxes) are connected to junctions (black dots). A name on a junction is not the name of the junction but the name of all ports connected to that junction.

\textbf{The storage components} spring $\mathtt{H_{s}}$ and mass $\mathtt{H_{m}}$ are connected to the Dirac structure $\mathtt{D_m}$ via ports $\mathtt{H_{s}.q}$ and $\mathtt{H_{m}.p}$, where the port variables are $(\mathtt{H_{s}.q.f}, \mathtt{H_{s}.q.e}) = (\dot{q}, \partial_q H)$ and $(\mathtt{H_{m}.p.f}, \mathtt{H_{m}.p.e}) = (\dot{p}, \partial_p H)$.

\textbf{The resistive structure} damping $\mathtt{R_{d}}$ is connected to the Dirac structure $\mathtt{D_m}$ via port $\mathtt{R_{d}.p}$ with port variables $(\mathtt{R_{d}.p.f}, \mathtt{R_{d}.p.e}) = (dv, v)$ and connected to the environment $\mathtt{E}$ via port $\mathtt{R_{d}.s_{e}}$ with port variables $(\mathtt{R_{d}.s_{e}.f}, \mathtt{R_{d}.s_{e}.e}) = (-\frac{1}{\theta_{0}}dv^2, \theta_{0}-\theta_{0})$. According to the prerequisite "isothermal", the damping process is in thermodynamic equilibrium with the environment. The net power at the damping $\mathtt{R_{d}.p.e} * \mathtt{R_{d}.p.f} + \mathtt{R_{d}.s_{e}.e} * \mathtt{R_{d}.s_{e}.f} = \mathtt{R_{d}.p.e} * \mathtt{R_{d}.p.f} = dv^2$ represents the dissipated power of the system.

The following relation defines the resistive structure $\mathtt{R_d}$:

\begin{equation}
    \label{eq:resistive_structure_idho}
    \left[\begin{array}{l}\mathtt{R_{d}.p.f} \\ \mathtt{R_{d}.s_{e}.f}\end{array}\right]=\frac{1}{\theta_0} d\left[\begin{array}{rr}\theta_0+\mathtt{R_{d}.s_{e}.e} & -\mathtt{R_{d}.p.e} \\ -\mathtt{R_{d}.p.e} & \frac{(\mathtt{R_{d}.p.e})^2}{\theta_0+\mathtt{R_{d}.s_{e}.e}}\end{array}\right]\left[\begin{array}{l}\mathtt{R_{d}.p.e} \\ \mathtt{R_{d}.s_{e}.e}\end{array}\right].
\end{equation}

\textbf{The Dirac structure $\mathtt{D_m}$} is connected to the ports $\mathtt{H_{s}.q}$, $\mathtt{H_{m}.p}$ and $\mathtt{R_{d}.p}$ and thus can be defined by

\begin{equation}
    \label{eq:Dirac_structure_idho}
    \mathcal{D}_m=\left\{ \left( \left[\begin{array}{l}\mathtt{H_{s}.q.f} \\ \mathtt{H_{m}.p.f} \\ \hline \mathtt{R_{d}.p.f} \end{array} \right], \left[\begin{array}{l} \mathtt{H_{s}.q.e} \\ \mathtt{H_{m}.p.e} \\ \hline \mathtt{R_{d}.p.e} \end{array}\right] \right) \in (T_{x}\mathcal{X} \times \mathcal{F}_R) \times (T_{x}^{*}\mathcal{X} \times \mathcal{E}_R)    \,\middle\vert\,    \left[\begin{array}{l}\mathtt{H_{s}.q.f} \\ \mathtt{H_{m}.p.f} \\ \hline \mathtt{R_{d}.p.e}\end{array}\right]=J\left[\begin{array}{l}\mathtt{H_{s}.q.e} \\ \mathtt{H_{m}.p.e} \\ \hline \mathtt{R_{d}.p.f} \end{array}\right]\right\},
\end{equation}

where $J$ is a skew-symmetric matrix

\begin{equation}
    \label{eq:skew-symmetric_matrix_idho}
    J = 
    \left[\begin{array}{rr|r}
    0 & 1 & 0 \\
    -1 & 0 & -1 \\
    \hline 0 & 1 & 0
    \end{array}\right].
\end{equation}

The above modular description can be reduced to the system of ODEs:

\begin{equation}
    \label{eq:ODE_isothermal_damped_harmonic_oscillator}
    \begin{bmatrix}
    \dot{q}\\
    \\
    \dot{p}\\
    \\
    \dot{s}\\
    \end{bmatrix}
    =
    \begin{bmatrix}
    \frac{p}{m}\\
    \\
    -\frac{q}{c}-d\frac{p}{m}\\
    \\
    \frac{1}{\theta_{0}} d v^2\\
    \end{bmatrix}.
\end{equation}


\clearpage
\chapter{Neural Networks}
\label{ch:chapter4}
Artificial neural networks (ANNs), also referred to simply as neural networks (NNs), are mathematical models that mimic the behavioral characteristics of animal neural networks for distributed parallel information processing, even though this mimicry is superficial \cite{russell2010artificial}.

\section{Perceptron}
Neural network technology originated in the 1960s as perceptron by Rosenblatt \cite{rosenblatt1958perceptron}. The characteristics of perceptrons are strongly contemporary: their inputs and outputs are in binary form \cite{mcculloch1943logical}. Each of these perceptrons is characterized as either "on" or "off", with an "on" response occurring when stimulated by a sufficient number of neighboring perceptrons.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/perceptron.pdf}
    \caption{Perceptron.}
    \label{fig:perceptron}
\end{figure}

In Figure \ref{fig:perceptron}, $x_i$ denotes multiple inputs to the perceptron, $w_i$ denotes the weights corresponding to each input and the arrow to the right indicates that the perceptron has one output. Each input is multiplied by the corresponding weight and then summed, and the result is compared with a threshold, with 1 being output if it is greater than the threshold and 0 being output if it is less than the threshold:

\begin{equation}
    \label{eq:perceptron_1}
    f(x)=\begin{cases}0 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i} \leq \text{threshold} \\ 1 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i}>\text{threshold}\end{cases}
\end{equation}

Let $b=-\text{threshold}$, the formula \ref{eq:perceptron_1} can be rewritten as:

\begin{equation}
    \label{eq:perceptron_2}
    f(x)=\begin{cases}0 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i}+b \leq 0 \\ 1 & \text{if } \sum\nolimits_{i=1}^n w_{i} x_{i}+b>1\end{cases},
\end{equation}

where b is also known as bias. 

\section{Activation Functions}
Following the idea of perceptron, a neuron in a neural network computes the weighted sum of inputs and then applies an activation function $g$ to yield the output $g(z)$, where $z=\sum\nolimits_{i=1}^n w_{i} x_{i} + b$. For instance, a neuron adopts the Heaviside step function (also known as binary step function) $g$ as its activation function defined by

\begin{equation}
    \label{eq:activation_function}
    g(z)=\begin{cases} 0 & \text{if } z\leq 1 \\ 1  & \text{if } z>0
    \end{cases}.
\end{equation}

The following figure depicts the structure of a neuron.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/activation_function.pdf}
    \caption{The structure of a classical neuron.}
    \label{fig:neuron_structure}
\end{figure}

An activation function should be differentiable, so that we can compute the gradient for training, cf. \cite{goodfellow2016deep}. In addition, the range of the activation function should be suitable, not too large or too small. Otherwise, it will affect the efficiency and stability of the training. Despite these requirements mentioned above, there are still a wide variety of activation functions available. We will introduce some of them in the following. For more details about activation functions, we refer to \cite{nwankpa2018activation}\cite{dubey2022activation}.

\textbf{The Sigmoid function} is defined by

\begin{equation}
    \label{eq:sigmoid}
    \sigma(z)=\frac{1}{1+\exp (-z)}.
\end{equation}

The Sigmoid function is a classical saturating function. The probability in real life is always limited to the range of 0 to 1, and this characteristic is consistent with the range of Sigmoid. Hence, it is common to use Sigmoid as activation function when the output is probabilistic \cite{nwankpa2018activation}. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/sigmoid.pdf}
    \caption{Sigmoid.}
    \label{fig:sigmoid}
\end{figure}

\textbf{The hyperbolic tangent function} is known as $\mathrm{Tanh}$ function, which is given by

\begin{equation}
    \label{eq:tanh}
    f(z)=\frac{\left(e^{z}-e^{-z}\right)}{\left(e^{z}+e^{-z}\right)}.
\end{equation}

Compared to Sigmoid, the $\mathrm{Tanh}$ function has the range of -1 to 1:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/tanh.pdf}
    \caption{$\mathrm{Tanh}$.}
    \label{fig:tanh}
\end{figure}

The $\mathrm{Tanh}$ function is also a a saturating function. The output of a saturating function is saturated for higher and lower inputs, which may lead to vanishing gradient problem. The gradient vanishing problem describes a situation, where the gradient of the objective function with respect to the parameters becomes close to zero. This situation results in almost no update in the parameters \cite{dubey2022activation}. As a result, the training is almost stopped.

\textbf{The Rectified Linear Unit (ReLU) function} is defined by

\begin{equation}
    \label{eq:ReLU}
    \mathrm{ReLU}(x) = max(0,x).
\end{equation}

The main advantage of using ReLU is that we only need to perform additions, multiplications and comparison operations, which are more efficient in computation than performing exponentials and divisions \cite{nwankpa2018activation}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/ReLU.pdf}
    \caption{ReLU.}
    \label{fig:ReLU}
\end{figure}


\section{Feedforward Neural Network Architecture}
There exists a variety of neural network architectures. In this thesis, we will focus on feedforward neural networks. A feedforward neural network can be seen as a directed acyclic graph (DAG) with designated input and output nodes, which are fully connected to each other, i.e., all nodes in the next layer are connected to each node in the previous layer \cite{russell2010artificial}. Each node computes its inputs from the previous layer with the parameters and activation function and passes the result to all nodes in the next layer as their inputs. By definition of DAG, these nodes will never form a closed loop. The following is an example of feedforward neural network architecture.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=1]{figures/feedforward_neural_network.pdf}
    \caption{An example of feedforward neural network architecture.}
    \label{fig:feedforward}
\end{figure}

To clarify the principle of feedforward neural network, we first fix some notations. $L$ refers to the number of layers (input layer not included) in the feedforward neural network, the index $l$ refers to the $l$-th layer and $M_{l}$ stands for the number of neurons in $l$-th layer. $g_{l}$ denotes the activation function in the $l$-th layer. The parameters are denoted by the weight matrix $\mathbf{W}^{(l)} \in \mathbb{R}^{M_{l} \times M_{l-1}}$ and the bias vector $\mathbf{b}^{(l)} \in \mathbb{R}^{M_{l}}$. Thus, the input $\mathbf{z}^{(l)} \in \mathbb{R}^{M_{l}}$ and the output $\mathbf{a}^{(l)} \in \mathbb{R}^{M_{l}}$ of $l$-th layer can be written as:

\begin{equation}
    \label{eq:input_output_neuron}
    \begin{aligned}
    \mathbf{z}^{(l)} &= \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}\\
    \mathbf{a}^{(l)} &= g_{l}(\mathbf{z}^{(l)}).
    \end{aligned}
\end{equation}

The feedforward neural network can be seen as a function $f$ according to the relation $\hat{\mathbf{y}} = f(\mathbf{x};\mathbf{\theta})$, where $\mathbf{x}$ is the input to the neural network and $\mathbf{\theta}$ is the set of the parameters $\theta=((\mathbf{W}^{(1)}, \mathbf{b}^{(1)}), ..., (\mathbf{W}^{(l)}, \mathbf{b}^{(l)}))$. Note that $\mathbf{z}^{(l)}$ represents the input to a hidden layer or an output layer (input layer not included), while $\mathbf{x}$ here represents the input to the neural network. From another perspective, $\mathbf{x}$ is the output of the input layer, i.e., $\mathbf{x}=[x_1, ..., x_n]=\mathbf{a}^{(0)}$ and $\hat{\mathbf{y}}$ is the output of the ouput layer as well as the output of the neural network, i.e., $\hat{\mathbf{y}}=f(\mathbf{x};\mathbf{\theta})=\mathbf{a}^{(L)}$. 

The following is a code example in Julia for feedforward propagation:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    using NNlib:sigmoid

    # 3 neurons in this layer and 2 neurons in the next layer
    # generate random parameters
    W1 = rand(2, 3)
    b1 = rand(2)
    # 2 neurons in this layer and 4 neurons in the next layer
    # generate random parameters
    W2 = rand(4, 2)
    b2 = rand(4)
    
    # input
    a1 = rand(3)
    # The first layer
    z2 = (W1 * a1) .+ b1
    a2 = sigmoid(z2)
    # The second layer
    z3 = (W2 * a2) .+ b2
    a3 = sigmoid(z3)
\end{minted}

\section{Training}

\subsection{Optimization}
Optimization theory is a branch of mathematics dedicated to solving optimization problems. In machine learning, an optimization problem is defined by a differentiable function called the loss function (or cost function), in which we want to minimize or maximize the function value. In general, finding the global optimum of a convex objective function is a relatively simple optimization problem. However, a lot of optimization problems in machine learning are formulated as non-convex optimization problems, in which the objective functions are non-convex \cite{sun2019survey}. It may be more complex to find the global optimum of a non-convex function. In this case, one may choose to settle for second best, i.e., seeking the local optimum.

\subsection{Data Set}
A data set is a set of samples (or data). In supervised learning, the data is commonly divided into two data sets: training set and test set. The training set is used to train the model, while the test set is used to evaluate the model. Such a data set (whether for training or for testing) can be denoted by $\mathcal{D}=\{ (\mathbf{x}_{n}, \mathbf{y}_{n}) \}_{n=1}^{N} = \{ (\mathbf{x}_{1}, \mathbf{y}_{1}),  (\mathbf{x}_{2}, \mathbf{y}_{2}), ..., (\mathbf{x}_{N}, \mathbf{y}_{N}) \}$, where $\mathbf{x}_{n}$ are inputs and $\mathbf{y}_{n}$ are target values.

\subsection{Loss Functions}
A loss function $L$ is a non-negative real-valued differentiable function that quantifies the error between the target value $\mathbf{y}$ and the estimated value $\hat{\mathbf{y}}$. 

Suppose that the goal of the optimization is to find the minimum. This minimum can be formulated as MSE (mean squared error):

\begin{equation}
    \label{eq:minimum}
    \min _\theta \frac{1}{N} \sum_{i=1}^N L\left(y^i, \hat{y}^i\right),
\end{equation}

where $L$ is the loss function. $L_{1}$ and $L_{2}$ loss function are commonly used.

\textbf{Absolute error loss function} is also known as $L_{1}$ loss function. It minimizes the sum of the absolute differences between the target value $\mathbf{y}$ and the estimated values $\hat{\mathbf{y}}$:

\begin{equation}
    \label{eq:AE_loss}
    L_{1}( \mathbf{y}, \hat{\mathbf{y}} ) = ||\mathbf{y}-\hat{\mathbf{y}}||^{2}_{1},
\end{equation}

where $|| \cdot ||_{1}$ is $\ell_1$ norm $\sqrt[1]{\sum_{i=1}^N | \cdot |^1}$.

\textbf{Squared error loss function} is also known as $L_{2}$ loss function. It minimizes the sum of the square of the differences between the target value $\mathbf{y}$ and the estimated values $\hat{\mathbf{y}}$:

\begin{equation}
    \label{eq:SE_loss}
    L_{2} ( \mathbf{y}, \hat{\mathbf{y}} ) = ||\mathbf{y}-\hat{\mathbf{y}}||^{2}_{2},
\end{equation}

where $|| \cdot ||_{2}$ is $\ell_2$ norm $\sqrt[2]{\sum_{i=1}^N | \cdot |^2}$.


\subsection{Batch Gradient Descent}
Gradient Descent (GD) is one of the most popular algorithms for performing optimization \cite{ruder2016overview}. In machine learning, Batch Gradient Descent (BGD) is an optimization algorithm designed to find the minimum of the objective function, but the points it finds are not guaranteed to be globally optimal. Occasionally, BGD may get stuck in local optima.

BGD computes the gradient of the loss function with respect to the parameters for the entire training set and performs an update of the parameters in the opposite direction of the gradient \cite{ruder2016overview}:

\begin{equation}
    \label{eq:GD}
    \theta_{t+1} = \theta_{t} - \alpha \frac{1}{N} \sum\nolimits_{i=1}^N  \frac{\partial L (y^i, \hat{y}^i)}{\partial \theta_{t}},
\end{equation}

where $\theta_{t}$ is the parameters at $t$ and $\alpha$ is a small real number called learning rate. BGD is guaranteed to converge to the global minimum for convex functions and to the local minimum for non-convex functions \cite{ruder2016overview}.

\subsection{Stochastic Gradient Descent}
In the process of BGD above, the objective function is the MSE over the whole training set. However, for large data sets, performing BGD leads to high computational complexity in each iteration \cite{sun2019survey}.

In order to reduce the computational complexity, it is also possible to randomly pick one sample in each iteration and compute the gradient of the loss function with respect to the parameters for this sample and update the parameters immediately. This approach is known as Stochastic Gradient Descent (SGD). For optimization problems involving non-convex objective function, the SGD algorithm may be able to escape from local optima and saddle points easier \cite{sun2019survey}.

\subsection{Automatic Differentiation}
Automatic differentiation (AD) is a set of techniques that allow a computer program to compute the derivative of a differentiable function. Automatic differentiation uses the fact that a function can be decomposed into a finite set of elementary operations where the derivatives are known. Then we can obtain the overall derivative by combining the derivatives of the elementary operations through the chain rule. For details, we refer to \cite{baydin2018automatic} and \cite{margossian2019review}.

\subsection{Backpropagation}
Backpropagation (also known as reverse-mode automatic differentiation) is an algorithm for efficient computation of gradients \cite{baydin2018automatic}.

Consider a neural network $f$. According to the chain rule, the gradient of the loss function with respect to the parameters $\mathbf{W}$ and $\mathbf{b}$ can be written as

\begin{equation}
    \label{eq:Chain_rule_partial}
    \begin{aligned}
        \frac{\partial L\left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{W}^{(l)}} &= \frac{\partial L \left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{z}^{(l)}} 
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}},
        \\
        \frac{\partial L\left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{b}^{(l)}} &= \frac{\partial L \left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{z}^{(l)}} 
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}},
    \end{aligned}
\end{equation}

where $\mathbf{z}^{(l)}$ is the input to the $l$-th layer, i.e., $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$. 

In the following, we compute the three terms: $\frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}}$, $\frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}}$ and $\frac{\partial L \left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{z}^{(l)}}$.

We compute the first two terms according to \ref{eq:input_output_neuron}:

\begin{equation}
    \label{eq:two_terms}
    \begin{aligned}
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}} &= \mathbf{a}^{(l-1)}
        \\
        \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}} &= \mathbf{I},
    \end{aligned}
\end{equation}

where $\mathbf{I} \in \mathbb{R}^{M_{l}}$ is an identity matrix. 

The third term $\frac{\partial L \left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{z}^{(l)}}$ is called error, which reflects the sensitivity of the loss to neurons in the $l$-th layer. The error is denoted by $\delta^{(l)}$. Applying the chain rule, the error from the previous layer $\delta^{(l)}$ in terms of the error in the next layer $\delta^{(l+1)}$ is given by

\begin{equation}
    \label{eq:error_in_one_layer}
    \begin{aligned}
        \delta^{(l)} &= \frac{\partial L \left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{z}^{(l)}}
        \\
        &= \frac{\partial L \left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{z}^{(l+1)}} \cdot \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \cdot \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}},
    \end{aligned}
\end{equation}

where $\mathbf{z}^{(l+1)} = \mathbf{W}^{(l+1)} \mathbf{a}^{(l)} + \mathbf{b}^{(l+1)}$ and $\mathbf{a}^{(l)} = g_{l}(\mathbf{z}^{(l)})$. And we can also obtain $\delta^{(l+1)}=\frac{\partial L \left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{z}^{(l+1)}}$, $\frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} = \mathbf{W}^{(l+1)}$ and $\frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} = g_{l}'(\mathbf{z}^{(l)})$.

Hence, the error can be written as

\begin{equation}
    \label{eq:error_in_one_line}
        \delta^{(l)} = (\mathbf{W}^{(l+1)})^T \delta^{(l+1)} \odot g_{l}'(\mathbf{z}^{(l)}),
\end{equation}

where $\odot$ refers to Hadamard product (also known as element-wise product) of two matrices with the same dimensions.

Note that $(\mathbf{W}^{(l+1)})^T$ in \ref{eq:error_in_one_line} is transposed, as the direction is backward (from $l+1$ to $l$).

Lastly, after we have the three terms, Equation \ref{eq:Chain_rule_partial} together with \ref{eq:two_terms} and \ref{eq:error_in_one_line} can be rewritten as

\begin{equation}
    \label{eq:Chain_rule_partial_deducted}
    \begin{aligned}
        \frac{\partial L\left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{W}^{(l)}} &= \delta^{(l)} (\mathbf{a}^{(l-1)})^T \quad \in \mathbb{R}^{M_{l} \times M_{l-1}}
        \\
        \frac{\partial L\left( \mathbf{y}, \hat{\mathbf{y}} \right)}{\partial \mathbf{b}^{(l)}} &= \delta^{(l)} \quad \in \mathbb{R}^{M_{l}}.
    \end{aligned}
\end{equation}

Thereafter, the gradient of the loss function w.r.t the paramters can be used to update the parameters through BGD or SGD. For more details about backpropragation, we refer to \cite{nielsen2015neural}.

\subsection{Adam Algorithm}
The Adam algorithm (Adaptive Moment Estimation Algorithm) \cite{kingma2014adam} is an algorithm that combines the momentum method \cite{qian1999momentum} and the RMSProp algorithm (Root Mean Squared Propagation algorithm) \cite{tieleman2012divide}. In practice, the Adam algorithm is relatively stable in the process of gradient descent. Hence, it can be applied for most non-convex optimization problems with large data sets and high dimensional space \cite{sun2019survey}. For more details about the above mentioned or other optimization algorithms, we refer to \cite{ruder2016overview}.

The Adam algorithm uses mini-batch gradient descent (MBGD) \cite{bottou2010large}. Consider a neural network $f$ and a training set $\mathcal{D}=\{ (\mathbf{x}_{n}, \mathbf{y}_{n}) \}_{n=1}^{N}$. The MBGD algorithm splits the training set $\mathcal{D}$ into a sequence of subsets (or mini batches) $\mathcal{S}_i=\{(\mathbf{x}_{m}, \mathbf{y}_{m}) \}_{m=1}^{M}$, where $M$ is the batch size.

Similar to the momentum method, Adam stores the exponentially decaying average of past gradients $m_t$. Moreover, similar to the Adadelta algorithm \cite{zeiler2012adadelta} and the RMSprop algorithm, Adam also stores the exponentially decaying average of past squared gradients $v_t$:

\begin{equation}
    \label{eq:moment_estimate}
    \begin{aligned}
        m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_{t},
        \\
        v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_{t} \odot g_{t},
    \end{aligned}
\end{equation}

where $v_t$ and $m_t$ can also be referred to as estimates of the first moment and the second moment of the gradients. And $g_{t}$ stands for the gradient of the loss function with respect to the parameters at $t$:

\begin{equation}
    \label{eq:mean_gradient}
    g_{t} = \frac{1}{M} \sum\nolimits_{j=1}^M \frac{\partial L (y^j, \hat{y}^j)}{\partial \theta_{t}}.
\end{equation}

$\beta_1$ and $\beta_2$ are decay rates. The authors of Adam proposed default values of 0.9 for $\beta_1$ and 0.999 for $\beta_2$. However, as $m_t$ and $v_t$ are initially set to vector of zeros, the authors of Adam observed that they are biased towards zero during the initial time when $\beta_1$ and $\beta_2$ are close to 1. Hence, instead of using the original $m_t$ and $v_t$, they use bias-corrected first and second moment estimates $\hat{m_t}$ and $\hat{v_t}$:

\begin{equation}
    \label{eq:corrected_moment_estimate}
    \begin{aligned}
        \hat{m_t} &= \frac{m_t}{1-\beta_1^t}
        \\
        \hat{v_t} &= \frac{v_t}{1-\beta_2^t}.
    \end{aligned}
\end{equation}

Following the idea of gradient descent, they perform an update of the parameters according to

\begin{equation}
    \label{eq:Adam_GD}
    \begin{aligned}
        \theta_{t+1} = \theta_{t} - \alpha \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon},
    \end{aligned}
\end{equation}

where $\epsilon$ is a very small constant (the authors of Adam proposed default value of $10^{-8}$ for it) for stabilization (avoiding zero as the denominator). 

The following is an example of using the Adam algorithm in Julia code:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
using NNlib:sigmoid
using Statistics: mean
using Flux
using Zygote: gradient

# Generate random parameters
W = rand(2, 3)
b = rand(2)
# Define the NN model
predict(x) = sigmoid((W * x) .+ b)
# Define the loss function
loss(x, y) = mean(abs2, ( predict(x) .- y))
# Compute the loss
input = rand(3, 100)
target_value = rand(2, 100)

# The ADAM algorithm with the learning rate α=0.01 and 
# decay rates β1=0.9, β2=0.999.
opt = Flux.Optimise.ADAM(0.01, (0.9, 0.999))
# Construct a function to compute the gradients
θ = Flux.params(W, b)
gs(x, y) = gradient(() -> loss(x, y), θ)
# Construct a dataloader.
dataloader = Flux.Data.DataLoader((input, target_value), batchsize = 10)

# Update the parameters with the given learning rate and optimization algorithm
for (x, y) in dataloader
    for θ in (W, b) 
        Flux.Optimise.update!(opt, θ, gs(x, y)[θ])
    end
    println("loss: ", loss(x, y))
end
\end{minted}


\subsection{Training}
This subsection provides an example of performing the training of a single layer neural network in Julia language.

Firstly, we generate random parameters and perform the feedforward propagation:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # random seed
    rng = Random.default_rng()
    Random.seed!(rng, 0)
    
    # generate random initial parameters
    W = rand(rng, 2, 3)
    b = rand(rng, 2)

    # Perform the feedforward propagation
    predict(x, W, b) = sigmoid((W * x) .+ b)
\end{minted}

The loss function to be optimized can be defined by:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # generate random training data
    input = rand(rng, 3)
    target = rand(rng, 2)
    
    # mean squared error
    loss(input, target, W, b) = mean(abs2, predict(input, W, b) .- target)
\end{minted}

In Julia, there are some automatic differentiation tools available such as "Zygote.jl", "ReverseDiff.jl", etc. In the following code snippet, we use "Zygote.jl" \cite{Zygote.jl-2018} to compute the gradient of the loss function with respect to the parameters:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # loss function with respect to parameters only
    fwd = (W, b) -> loss(input, target, W, b)
    gradW, gradb = gradient(fwd, W, b)
\end{minted}

The last step is to update the parameters using gradient descent:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # parameter update using gradient descent
    α = 0.1 # learning rate
    W = W - α * gradW
    b = b - α * gradb
\end{minted}

An alternative for optimization is to use the function "Flux.Optimise.update!" from the deep learning framework "Flux.jl" \cite{Flux.jl-2018} \cite{innes:2018}. It provides a common interface for selecting various optimization algorithms. In the following, we simply select "Descent()" with learning rate 0.1:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # The optimization algorithm "Gradient Descent"
    opt = Flux.Descent(0.1)
    # Update the parameters with the given learning rate and optimization algorithm
    for θ in (W, b)
        Flux.Optimise.update!(opt, θ, gs[θ])
    end
    loss(input, target_value)
\end{minted}

Finally, we post the complete code block:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    using Random
    using NNlib: sigmoid
    using Statistics: mean
    using Zygote: gradient
    
    # random seed
    rng = Random.default_rng()
    Random.seed!(rng, 0)
    
    # perform the feedforward propagation for a hand-rolled single-layer NN
    predict(x, W, b) = sigmoid((W * x) .+ b)
    
    # generate random initial parameters
    W = rand(rng, 2, 3)
    b = rand(rng, 2)
    
    # generate random training data
    input = rand(rng, 3)
    target = rand(rng, 2)
    
    # mean squared error
    loss(input, target, W, b) = mean(abs2, predict(input, W, b) .- target)
    
    # value of loss function before training
    loss1 = loss(input, target, W, b)
    
    # parameter update using gradient descent
    α = 0.1 # learning rate
    # loss function with respect to parameters only
    fwd = (W, b) -> loss(input, target, W, b)
    gradW, gradb = gradient(fwd, W, b)
    W = W - α * gradW
    b = b - α * gradb

    # value of loss function after training
    loss2 = loss(input, target, W, b)
\end{minted}


\clearpage
\chapter{Neural ODEs}
\label{ch:chapter5}
To obtain more accurate results from a neural network model, we may think of stacking more hidden layers. However, when a neural network model is built with hundreds of hidden layers, they found that the vanishing or exploding gradient problem often occurs during backpropagation \cite{glorot2010understanding}. Due to the vanishing or exploding gradient problem, a deeper neural network could even yield worse results than a shallower one. Residual Neural Network (ResNet) was exactly designed to tackle these problems \cite{he2016deep}. The proposers of ResNet argue that the results of deeper neural networks should be better or at least equal to the results of shallower neural networks, but should not be worse.

\section{Residual Neural Networks}
Usually, the output of a layer of a neural network only directly affects the next layer. However, ResNet breaks this convention and allows the input $\mathbf{x}$ to skip several weight layers $F(\mathbf{x};\theta)$. $H$ is a target function given by

\begin{equation}
    \label{eq:ResNet}
    H(\mathbf{x})=\mathbf{x}+F(\mathbf{x};\theta).
\end{equation}

The following residual building block illustrates the idea of ResNet:

\clearpage
\begin{figure}[htbp!]
    \centering
    \includegraphics[scale=1]{figures/ResNet.pdf}
    \caption{Residual building block.}
    \label{fig:ResNet}
\end{figure}

The weight layers can be thought of as serveral hidden layers that are skipped over by a shortcut connection. This shortcut connection changes the learning target from learning the original target $H(\mathbf{x})$ to the residual $H(\mathbf{x})-\mathbf{x}$. The proposers of ResNet hypothesized that the residual is easier to optimize than learning the original output and then verified this hypothesis in their experiments. 

 The significance of ResNet is that it provides a new direction to address the challenges of stacking more and more hidden layers to the neural networks. Benefiting from the ResNet, the vanishing or exploding gradient problem can be solved to some extent.

\section{Neural ODEs}
The hidden layers in a ResNet are built by a sequence of transformations like $H(\mathbf{x})=\mathbf{x}+F(\mathbf{x};\theta)$. This sequence of transformations was found to be remarkably similar to the Euler method formula $x_{n+1} = x_{n} + h \cdot f_{n}$ \cite{ruthotto2020deep}. In view of this, the transformation \ref{eq:ResNet} can also be formulated as

\begin{equation}
    \label{eq:Euler_discretization}
    \mathbf{z}_{t+1} = \mathbf{z}_{t} + f(\mathbf{z}_{t}; \theta),
\end{equation}

where $t \in \{t\}_{t=0}^T$.

Then, we formulate this transformation as 

\begin{equation}
    \label{eq:Euler_discretization_rewritten}
    \frac{\mathbf{z}_{t+1}-\mathbf{z}_{t}}{\Delta t} = f(\mathbf{z}_{t}; \theta),
\end{equation}

where $\Delta t = (t+1)-t = 1$. If $\Delta t$ is a very small step, we rewrite \ref{eq:Euler_discretization_rewritten} as

\begin{equation}
    \label{eq:Euler_discretization_small_step}
    \lim_{\Delta t \to 0} \frac{\mathbf{z}_{t+1}-\mathbf{z}_{t}}{\Delta t} = f(\mathbf{z}_{t}; \theta).
\end{equation}

The LHS of \ref{eq:Euler_discretization_small_step} is the derivative of $\mathbf{z}(t)$:

\begin{equation}
    \label{eq:Neural_ODE}
    \frac{d\mathbf{z}(t)}{dt} = f(\mathbf{z}(t), t; \theta).
\end{equation}

Equation \ref{eq:Neural_ODE} is called Neural Ordinary Differential Equation (Neural ODE) \cite{chen2018neural}.

Plugging \ref{eq:Neural_ODE} into \ref{eq:Euler_discretization}, we obtain the transformation 

\begin{equation}
    \label{eq:Euler_transformation_NeuralODE}
    \mathbf{z}(t+\varepsilon ) = \mathbf{z}(t) + \int_{t}^{t+\varepsilon } f(\mathbf{z}(t), t, \theta)dt,
\end{equation}

where $\varepsilon $ is a small time step size and $f$ is a neural network. An ODE solver is treated as a black box that provides a solution to an IVP, which consists of a Neural ODE and its initial state. Thus, the loss function is

\begin{equation}
    \label{eq:Neural_ODE_Gradient_loss}
    \begin{aligned}
    L(\mathbf{z}(t+\varepsilon )) &= L(\mathbf{z}(t) + \int_{t}^{t+\varepsilon } f(\mathbf{z}(t), t, \theta)dt \:)\\
    &= L(ODESolver(\mathbf{z}(t), f, t, t+\varepsilon , \theta)).
    \end{aligned}
\end{equation}

As stated before, to optimize a loss function through backpropagation, we compute the gradient of the loss function w.r.t the parameters. Before that, we need to compute the error like we did in \ref{eq:error_in_one_layer}:

\begin{equation}
    \label{eq:Neural_ODE_error}
    \frac{d L}{d\mathbf{z}(t)} = \frac{d L}{d\mathbf{z}(t+\varepsilon )} \cdot \frac{d\mathbf{z}(t+\varepsilon )}{d\mathbf{z}(t)}.
\end{equation}

However, the main difficulty is to perform backpropagation through the ODE solver. To address this issue, the authors of Neural ODEs propose to use the adjoint method.


\section{Adjoint Method}
The adjoint method (or adjoint sensitivity method) is a method designed to compute the gradients of functions by solving ODEs, which can be dated back to the 1960s \cite{boltyanskiy1962mathematical}. During the training of a neural network with many layers, computing the gradient of the loss function through backpropagation leads to a huge memory cost. The significance of using the adjoint method is that the gradient of the loss function can still be computed efficiently without storing the intermediate activations and thus the memory cost can be reduced.

For Neural ODEs, the goal of using the adjoint method is to compute the gradient of the loss function with respect to the state $\frac{d L}{d\mathbf{z}(t)}$ and the gradient of the loss function with respect to the parameters $\frac{d L}{d\theta}$.

To use the adjoint method, the first step is to define an adjoint state that is equal to the gradient of the loss function with respect to the state:

\begin{equation}
    \label{eq:adjoint_state}
    \mathbf{a}(t) = \frac{d L}{d\mathbf{z}(t)}.
\end{equation}

And then, we expand the Taylor series for the state $\mathbf{z}(t+\varepsilon )$ at point $t$:

\begin{equation}
    \label{eq:taylor_series_expansion}
    \begin{aligned}
    \mathbf{z}(t+\varepsilon ) &= \mathbf{z}(t) + \int_{t}^{t+\varepsilon } f(\mathbf{z}(t), t, \theta)dt\\
    &= \mathbf{z}(t) + \varepsilon f(\mathbf{z}(t), t; \theta) + O (\varepsilon ^2).
    \end{aligned}
\end{equation}

Then plug Equation \ref{eq:adjoint_state} and \ref{eq:taylor_series_expansion} into Equation \ref{eq:Neural_ODE_error}, we obtain

\begin{equation}
    \label{eq:Neural_ODE_adjoint_state}
    \mathbf{a}(t) = \mathbf{a}(t+\varepsilon ) \cdot \frac{\partial}{\partial \mathbf{z}(t)} (\mathbf{z}(t) + \varepsilon f(\mathbf{z}(t), t; \theta) + O (\varepsilon ^2)) .
\end{equation}

By definition of differentiation, the time derivative of the adjoint state follows:

\begin{equation}
    \label{eq:adjoint_state_gradient_hidden_state}
    \begin{aligned}
        \frac{d\mathbf{a}(t)}{dt} &= \lim_{\varepsilon \to 0+} \frac{\mathbf{a(t+\varepsilon)}-\mathbf{a(t)}}{\varepsilon} \\
        &= \lim_{\varepsilon \to 0+} \frac{\mathbf{a(t+\varepsilon)}-\mathbf{a}(t+\varepsilon ) \cdot \frac{\partial}{\partial \mathbf{z}(t)} (\mathbf{z}(t) + \varepsilon f(\mathbf{z}(t), t; \theta) + O (\varepsilon ^2)) }{\varepsilon}\\
        &= \lim_{\varepsilon \to 0+} \frac{-\varepsilon \mathbf{a(t+\varepsilon)} \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)} + O (\varepsilon ^2)}{\varepsilon}\\
        &= \lim_{\varepsilon \to 0+} - \mathbf{a(t+\varepsilon)} \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)} + O (\varepsilon)\\
        &= - \mathbf{a(t)} \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)}
    \end{aligned}
\end{equation}

We compute the initial adjoint state $\mathbf{a}(T) = \frac{d L}{d\mathbf{z}(T)}$. This can be considered as the initial state of an IVP problem. 

To obtain $\mathbf{a}(T-\varepsilon)$, we use the transformation

\begin{equation}
    \label{eq:adjoint_state_ODE}
    \begin{aligned}
    \mathbf{a}(T-\varepsilon) &= \mathbf{a}(T) + \int_{T}^{T-\varepsilon} \frac{d\mathbf{a(t)}}{dt} \\
    &= \mathbf{a}(T) - \int_{T}^{T-\varepsilon} \mathbf{a(t)}^T \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{z}(t)}. \quad \text{plugging} \: \ref{eq:adjoint_state_gradient_hidden_state} \\ 
    \end{aligned}
\end{equation}

Note that the adjoint state $\mathbf{a(t)}^T$ is transposed due to the "backward" direction.

Similar to \ref{eq:adjoint_state}, the gradient of the loss function with respect to the parameters is defined by

\begin{equation}
    \label{eq:adjoint_state_parameters}
    \mathbf{a_\theta}(t) = \frac{d L}{d\theta}.
\end{equation}

The corresponding gradient of the adjoint state is of the form:

\begin{equation}
    \label{eq:adjoint_state_gradient_parameters}
    \frac{d L}{d\theta} = - \int_{T}^{0} \mathbf{a(t)}^T \frac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \mathbf{\theta}} dt .
\end{equation}


More details for $\frac{d L}{d\theta}$ can be found in the original paper \cite{chen2018neural}.

A short summary: from the above procedures, it can be concluded that the idea of the adjoint method in Neural ODEs is to compute a sequence of adjoint states in a "backward" direction by solving ODEs, and therefore the gradients can also be computed in an indirect manner.


\clearpage
\chapter{Structured Neural ODEs}
\label{ch:chapter6}
\section{Physics Priors}
In machine learning, some optimization algorithms tend to make some assumptions to improve the learning efficiency, and these assumptions are referred to as inductive biases (also known as priors) \cite{mitchell1997machine}. Inductive biases or priors may have different names in specific fields. In the field of physics, they are referred to as physics priors or physically informed inductive biases.

The experiments in \cite{gupta2019general} introduced a framework to model the Lagrangian and the generalized forces of mechanical systems by using neural networks. Their experiments show the advantage of a grey-box model endowed with physics priors in terms of data efficiency compared to a black-box model without prior knowledge. Another similar work related to Lagrangian is \cite{lutter2019deep}.

In the field of Hamiltonian mechanics, \cite{greydanus2019hamiltonian} proposed an approach to learn a parametric Hamiltonian function $H_{\theta}$ from the time derivatives of coordinates in the way that $\frac{\partial \mathcal{H}_{\theta}}{\partial \mathbf{p}} - \frac{d\mathbf{q}}{dt} = 0, \frac{\partial \mathcal{H}_{\theta}}{\partial \mathbf{q}} - \frac{d\mathbf{p}}{dt} = 0 $. Such an approach allows Hamiltonian Neural Networks (HNNs) to learn a conserved quantity that is analogous to the total energy.

\section{Neural Networks based on Neural ODEs}
In \cite{chen2019symplectic}, two neural network models of Hamiltonian systems are compared: O-NETs (ODE Neural Networks) and H-NETs (Hamiltonian Neural Networks), where H-NETs follow the idea of HNNs. 

Recall that a Neural ODE \ref{eq:Neural_ODE} contains a neural network $f$ on its RHS. An ODE solver provides a solution to the ODE such that $\mathbf{z}(t+\varepsilon) = ODESolver(\mathbf{z}(t), f, t, t+\varepsilon , \theta)$. Note that in \cite{chen2019symplectic}, $f$ is considered as a neural network (e.g. in the case of O-NETs). However, $f$ can also be a function that contains a neural network (e.g. in the case of HNNs or H-NETs). All neural network models here are based on Neural ODEs.

For better understanding, We will use Julia code to explain the central ideas of O-NET and H-NET in the following.

\subsection{O-NETs}
An O-NET is a neural network $f_{\theta}$ on the RHS of a Neural ODE. The training of an O-NET can be conceptually divided into five steps.

\textbf{Step 1}: construct a neural network

We construct a 2-inputs and 2-outputs O-NET using the deep learning framework "Flux.jl" or "Lux.jl" \cite{pal2022lux}.
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # Dense: construct a layer. For instance, Dense(2, 40, tanh) constructs 
    # a 2-input and 40-output layer with the activation function tanh.
    # Chain: connect layers.
    # O_NET: a feedforward neural network with 2 neurons in the input layer,
    # 40 neurons in the first hidden layer, 40 neurons in the second hidden 
    # layer and 2 neurons in the output layer.

    O_NET = Chain(Dense(2, 40, tanh),
                  Dense(40, 40, tanh),
                  Dense(40, 2))
\end{minted}

If we use both "Flux.jl" and "Lux.jl" simultaneously, then it is advisable to specify them explicitly, as they both contain chain and dense functions under same names. However, the ways of generating initial parameters and obtaining output are different in both.

We construct a neural network using "Flux.jl" like
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    O_NET = Flux.Chain(Flux.Dense(2, 40, tanh),
                       Flux.Dense(40, 40, tanh),
                       Flux.Dense(40, 2))
    # ps: the initial parameters of the neural network. 
    # re: a method to reconstruct the neural network with the given parameters ps 
    # and input x, e.g., re(ps)(x) is the output of the neural network with 
    # the given parameters ps and input x.
    ps, re = Flux.destructure(O_NET)
\end{minted}

or in "Lux.jl" like
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    O_NET = Lux.Chain(Lux.Dense(2, 40, tanh),
                      Lux.Dense(40, 40, tanh),
                      Lux.Dense(40, 2))
    # "Random.default_rng" is a random number generator. 
    # It generates a random number in preparation for 
    # generating random parameters in the next code line.
    using Random
    rng = Random.default_rng()
    # ps: the initial parameters of the neural network.
    # st: the state of the neural network. 
    ps, st = Lux.setup(rng, O_NET)
\end{minted}

\textbf{Step 2}: construct an IVP

Suppose that $\{ \mathbf{z}_{t} \}_{t=1}^{T}$ is a training set, where $\mathbf{z}_{t}$ are some discrete points from a trajectory observation. Given an initial state $\mathbf{z}_{0}$ and time steps $\{ t \}_{t=0}^{T}$, we define an IVP

\begin{equation}
    \label{eq:O-NET_IVP}
    \dot{\mathbf{z}}_t = f_{\theta}(\mathbf{z_t}), \quad \mathbf{z}(t_{0}) = \mathbf{z}_{0},
\end{equation}

where $f_{\theta}$ is an O-NET.

$f_{\theta}(\mathbf{z_t})$ at time $t$ estimates the time derivative of coordinates $\dot{\mathbf{z}}_{t}$ such that $\dot{ \mathbf{z}}_{t} = f_{\theta}(\mathbf{z}_t)$. In the case of canonical Hamiltonian systems, let $\mathbf{z}_{t} = [q_{t}, p_{t}]$, we can rewrite the IVP \ref{eq:O-NET_IVP} as $[\dot q_{t}, \dot p_{t}] = f_{\theta}(q_t, p_t)$, $\mathbf{z}_{0}= [q_{0}, p_{0}]$.

Let $\mathbf{z}_{0}=[1.0, 1.0]$ and $\{ t \}_{t=0}^{19.9} = (0.0, 0.1, 19.9)$, we construct this IVP with the following code snippet:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # dz is the time derivative of z at a fixed time.
    function ODE(dz, z, θ, t)
    # In Flux.jl, re(θ)(z) is the output of O-NET with the given parameters θ and
    # input z. In Lux.jl, this term should be rewritten as O_NET(z, θ, st).
    dz[1] = re(θ)(z)[1]
    dz[2] = re(θ)(z)[2]
    end
    
    initial_state = [1.0, 1.0]
    
    # Starting at 0.0 and ending at 19.9, the length of each single step is 0.1.
    # Thus, we have 200 time steps in total.
    time_span = (0.0, 19.9)
    time_steps = range(0.0, 19.9, 200)
    
    θ = ps
    
    # ODEProblem is an IVP constructor in the Julia package SciMLBase.jl
    using SciMLBase
    IVP = SciMLBase.ODEProblem(ODEFunction(ODE), initial_state, time_span, θ)
\end{minted}

\textbf{Step 3}: solve the IVP

To obtain the estimate of the coordinates trajectories $\{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T}$, we use an ODE solver to yield the solution to the IVP \ref{eq:O-NET_IVP}

\begin{equation}
    \label{eq:O-NET_ODESolver}
    \{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T} = ODESolver(\mathbf{z}_{0}, f_{\theta}, \{ t \}_{t=1}^{T}).
\end{equation}

To solve the IVP \ref{eq:O-NET_IVP} in Julia code, we use the package "CommonSolve.jl", which provides a common interface for distinct ODE solvers:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # Select a numerical method to solve the IVP
    using OrdinaryDiffEq
    numerical_method = ImplicitMidpoint()
    
    # Select the adjoint method to computer the gradient of the loss 
    # with respect to the parameters. ReverseDiffVJP is a callable 
    # function in the package SciMLSensitivity.jl, it uses 
    # the automatic differentiation tool ReverseDiff.jl to compute 
    # the vector-Jacobian products (VJP) efficiently. 
    using SciMLSensitivity
    sensitivity_analysis = InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))
    
    # Use the ODE Solver CommonSolve.solve to yield solution. 
    # And the solution is the estimate of the coordinates trajectories.
    using CommonSolve
    solution = CommonSolve.solve(IVP, numerical_method, p=θ, tstops = time_steps, sensealg=sensitivity_analysis)
    
    # Convert the solution into a 2D-array
    pred_data = Array(solution)
\end{minted}

The variable "pred\_data" is a 2D-array, which stands for the estimate of the coordinates trajectories $\{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T}$. In our case, we have 2 coordinates (q, p) in the Hamiltonian system and 200 time steps (from 0.0 to 19.9 with the step size 0.1). Thus, the variable "pred\_data" is a $2 \times 200$ array.

\textbf{Step 4}: construct a loss function

To train the neural network model, we define a $L_{2}$ loss function like \ref{eq:SE_loss} according to the relation

\begin{equation}
    \label{eq:loss_O-NET}
     L(\{ \mathbf{z}_{t} \}_{t=1}^{T}, \{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T}) = \lVert \{ \mathbf{z}_{t} \}_{t=1}^{T} - \{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T} \rVert_{2}^{2},
\end{equation}

where $\hat{\mathbf{z}_{t}}$ is the estimated state at time $t$ ($\hat{\mathbf{z}_{t}}$ is a point in the estimate of the coordinates trajectories $\{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T}$) and $\mathbf{z}_{t}$ is assumed to be a point from observation at time $t$ (${\mathbf{z}_{t}}$ is a point in the training set $\{ \mathbf{z}_{t} \}_{t=1}^{T}$). The goal of the optimization is to minimize the error between the points from the estimates and the points from the training set.

In code, we adopt the set $\{ \mathbf{z}_{t} \}_{t=1}^{T}$ generated by a system of ODEs instead of real observations. For instance, the training set for an undamped harmonic oscillator can be generated by the evolution of $q$ and $p$ according to \ref{eq:ODE_undamped_harmonic_oscillator}:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    # The system of ODEs of an undamped harmonic oscillator
    function ODEfunc_udho(dz, z, params, t)
      q, p = z
      m, c = params
      dz[1] = p/m
      dz[2] = -q/c
    end
    # params = [m, c]
    params = [2, 1] 
    prob = ODEProblem(ODEFunction(ODEfunc_udho), initial_state, time_span, params)
    ode_data = Array(CommonSolve.solve(prob, ImplicitMidpoint(), tstops = time_steps))
\end{minted}

The "ode\_data" is the set $\{ \mathbf{z}_{t} \}_{t=1}^{T}$. The loss computed by \ref{eq:loss_O-NET} is the sum of the square of the differences between the target values "ode\_data" $\{ \mathbf{z}_{t} \}_{t=1}^{T}$ and the estimated values "pred\_data" $\{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T}$.

Now we construct the loss function in code:
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
function solve_IVP(θ, batch_timesteps)
    IVP = SciMLBase.ODEProblem(ODEFunction(ODE), initial_state, (batch_timesteps[1], batch_timesteps[end]), θ)
    pred_data = Array(CommonSolve.solve(IVP, ImplicitMidpoint(), p=θ, tstops = batch_timesteps, sensealg=sensitivity_analysis))
    return pred_data
end

function loss_function(θ, batch_data, batch_timesteps)
    pred_data, _ = solve_IVP(θ, batch_timesteps)
    # "batch_data" is a batch of ode data
    loss = sum((batch_data .- pred_data) .^ 2)
    return loss, pred_data
end
\end{minted}

\textbf{Step 5}: train the neural network

There are different optimization algorithm can be used to train the model. We will use one of the most popular algorithms, the Adam Algorithm, in the following. As stated before, the Adam algorithm uses mini-batch gradient descent. Hence, first create an iterable object "dataloader" to load mini-batches.

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
# The dataloader generates a batch of data according to 
# the given batchsize from the "ode_data".
using Flux: DataLoader
dataloader = DataLoader((ode_data, time_steps), batchsize = 50)
\end{minted}

In order to obtain a satisfying result, we train the model multiple times. And we use a Julia package "Optimization.jl", which is an unified interface for different optimization algorithms and automatic differentiation tools.

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}

# Select an automatic differentiation tool
using Optimization
adtype = Optimization.AutoZygote()
# Construct an optimization problem with the given automatic differentiation 
# and the initial parameters θ
optf = Optimization.OptimizationFunction((θ, ps, batch_data, batch_timesteps) -> loss_function(θ, batch_data, batch_timesteps), adtype)
optprob = Optimization.OptimizationProblem(optf, θ)
# Train the model multiple times. The "ncycle" is a function 
# in the package IterTools.jl, it cycles through the dataloader "epochs" times.
using OptimizationOptimisers
using IterTools
epochs = 100
result = Optimization.solve(optprob, Optimisers.ADAM(0.01), ncycle(dataloader, epochs))
# Access the trained parameters
result.u
\end{minted}

The training is stopped when the given number of consecutive epochs run out. And the "Optimization.jl" package provides a checkpoint strategy that the parameters of the optimally tuned model are restored and saved. For instance, if the epochs is 100, then only the parameters corresponding to the minimal loss within 100 consecutive epochs will be saved eventually.  The saved parameters can be used for further training and for evaluating the model by the loss over the whole training set or a test set:
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
# The "loss_function" returns a tuple, where the first element of the tuple 
# is the loss
loss = loss_function(result.u, ode_data, time_steps)[1]
\end{minted}

If the loss is already small, we can choose to stop training early. Naturally, we can also continue the training, but since the loss function has converged, it does not make much sense to continue.

Finally, we use the neural network model with the optimized parameters to predict.

We use the neural network model in "Flux.jl" like
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    re()(initial_state)
\end{minted}

or in "Lux.jl" like
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    O_NET(initial_state, result.u, st)
\end{minted}


\subsection{H-NETs}
A H-NET is a neural network $\mathcal{H}_{\theta}$ that learns the Hamiltonian. The idea of H-NET is to replace the O-NET $f_{\theta}$ in \ref{eq:O-NET_IVP} with the estimate of the symplectic gradient $X_{\mathcal{H}_{\theta}}=(\frac{\partial \mathcal{H}_{\theta}}{\partial \mathbf{p}}, - \frac{\partial \mathcal{H}_{\theta}}{\partial \mathbf{q}})$. Then, the IVP \ref{eq:O-NET_IVP} can be rewritten as

\begin{equation}
    \label{eq:H-NET_IVP}
    \dot{\mathbf{z}}_t = X_{\mathcal{H}_{\theta}}(\mathbf{z_t}), \quad \mathbf{z}(t_{0}) = \mathbf{z}_{0}.
\end{equation}

The corresponding solution can be computed by

\begin{equation}
    \label{eq:H-NET_ODESolver}
     \{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T}  = ODESolver(\mathbf{z}_{0}, X_{\mathcal{H}_{\theta}}, \{ t \}_{t=1}^{T}),
\end{equation}

where $\mathcal{H}_{\theta}$ is a H-NET.

In code, the major difference between H-NET and O-NET appears in step 2. The Neural ODE of H-NET contains the estimate of the symplectic gradient $X_{\mathcal{H}_{\theta}}=(\frac{\partial \mathcal{H}_{\theta}}{\partial \mathbf{p}}, - \frac{\partial \mathcal{H}_{\theta}}{\partial \mathbf{q}})$ of a neural network on its RHS rather than the neural network itself:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
    using FiniteDiff
    function SymplecticGradient(NN, ps, st, z) 
      # Compute the gradient of the neural network
      ∂H = FiniteDiff.finite_difference_gradient(x -> sum(NN(x, ps, st)[1]), z)
      # Return the estimate of symplectic gradient
      return cat(∂H[2:2, :], -∂H[1:1, :], dims=1)
    end

    function ODE(z, θ, t)
        # Compute the estimate of symplectic gradient
        dz = vec(SymplecticGradient(H_NET, θ, st, z))
    end
\end{minted}

Note that H-NET is a 2-input and 1-output model, while O-NET is a 2-input and 2-output model:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
H_NET = Lux.Chain(Lux.Dense(2, 40, tanh),
                  Lux.Dense(40, 20, tanh),
                  Lux.Dense(20, 1))
\end{minted}

\subsection{HNNs}
A subtle difference between H-NET and HNNs is that HNNs use the loss function given by

\begin{equation}
    \label{eq:loss_HNNs}
    L_{HNN}(\{ \dot{\mathbf{z}_{t}} \}_{t=1}^{T}, \{ X_{\mathcal{H}_{\theta}}(\mathbf{z}_{t}) \}_{t=1}^{T}) = \lVert \{ \dot{\mathbf{z}_{t}} \}_{t=1}^{T} - \{ X_{\mathcal{H}_{\theta}}(\mathbf{z}_{t}) \}_{t=1}^{T} \rVert_{2}^{2},
\end{equation}

while H-NET uses the loss function given by $ L(\{ \mathbf{z}_{t} \}_{t=1}^{T}, \{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T}) = \lVert \{ \mathbf{z}_{t} \}_{t=1}^{T} - \{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T} \rVert_{2}^{2} $. It means that H-NET learns dynamics from the coordinates directly rather than from the time derivative of coordinates.

The loss function for H-NET corresponds to the one for O-NET:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
function solve_IVP(θ, batch_timesteps)
  IVP = SciMLBase.ODEProblem(ODEFunction(ODE), initial_state, (batch_timesteps[1], batch_timesteps[end]), θ)
  pred_data = Array(CommonSolve.solve(IVP, Midpoint(), p=θ, saveat = batch_timesteps, sensealg=sensitivity_analysis))
  return pred_data
end

function loss_function(θ, batch_data, batch_timesteps)
  pred_data = solve_IVP(θ, batch_timesteps)
  loss = sum((batch_data .- pred_data) .^ 2)
  return loss, pred_data
end

dataloader = Flux.Data.DataLoader((ode_data, time_steps), batchsize = 50)
\end{minted}

The loss function in HNNs omits the procedure of solving the IVP and computes the estimate of symplectic gradient directly. Hence, the "dataloader" loads the training set rather than timesteps (timesteps will be used to solve the IVP, however, the loss function in HNNs does not solve the IVP).

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
# Generate the time derivatives of the coordinates
dq_data = ode_data[2,:]/params[1]
dp_data = -ode_data[1,:]/params[2]
dq_data = reshape(dq_data, 1, :)
dp_data = reshape(dp_data, 1, :)
dz_data = cat(dq_data, dp_data, dims = 1)

function loss_function(θ, batch_data, batch_dz_data)
  pred_data = SymplecticGradient(H_NET, θ, st, batch_data)
  loss = sum((batch_dz_data .- pred_data) .^ 2)
  return loss, pred_data
end

# (ode_data, dz_data) is the whole training set.
dataloader = Flux.Data.DataLoader((ode_data, dz_data), batchsize = 50)
\end{minted}

\section{Structured Neural ODEs}
In Neural ODE, a neural network $f_{\theta}$ is considered as a pure black box. Such a pure data-driven modeling approach lacks robustness and provides no guarantees of convergence in small data regime (small data sets) \cite{raissi2017physics}. How about big data? In the field of physics, especially for some complex systems, the acquisition of data from observations is often costly. Therefore, it is also not easy to obtain big data sets. Nevertheless, in the field of physics, there exists a large accumulation of physics laws. In structured Neural ODEs, we encode physical laws as some prior information and feed them into the model to compensate for the lack of data.

\subsection{Physics Informed Function}

The centerpiece of structured Neural ODEs is to replace the neural network $f_{\theta}$ in \ref{eq:O-NET_IVP} with a new function $h_{\theta}$ given by the relation $\dot{\mathbf{z}}_t = h_{\theta}(f_{\theta}, \mathbf{z}_t)$. This new function $h_{\theta}$ is a function that consists of a neural network and some prior information, which may lead the training into the correct direction. Moving in the correct direction may help the loss function converge faster, even in the case of small data.

\subsection{Structured ODE Neural Network}

A Neural ODE with a physics informed function $h_{\theta}$ on its RHS is a structured Neural ODE, where its structure composes the known parts and unknown parts. Together with the initial state, we construct the IVP

\begin{equation}
    \label{eq:structured_neura_IVP}
    \dot{\mathbf{z}}_t = h_{\theta}(f_{\theta}, \mathbf{z}_t), \quad \mathbf{z}(t_{0}) = \mathbf{z}_{0},
\end{equation}

where $f_{\theta}$ can be called structured ODE neural network.

The solution to the IVP \ref{eq:structured_neura_IVP} can be computed via an ODE solver:

\begin{equation}
    \label{eq:structured_O-NET_ODESolver}
        \{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T} = ODESolver(\mathbf{z}_{0}, h_{\theta}, \{ t \}_{t=1}^{T}).
\end{equation}

\subsection{Example: Undamped Harmonic Oscillator}

Consider an undamped harmonic oscillator and its state variables $\mathbf{z}_t=(q_t, p_t)$ at time $t$. The known part is a part of a system of ODEs, e.g. $\dot{q}_t = p_t/m$, while the unknown part is replaced by the output of a neural network $\dot{p}_t = f_{\theta}(q_t)$. 

$h_{\theta} \big(f_{\theta}(q_t), p_t/m \big)$ at a fixed time $t$ estimates the time derivative of coordinates $\dot{\mathbf{z}_{t}}$ such that $\dot{\mathbf{z}_{t}} = [\dot{q}_{t}, \dot{p}_{t}] = h_{\theta} \big(f_{\theta}(q_t), p_t/m \big)$.

In the O-NET section, we consider the whole RHS as unknown part and define the ODE like:
\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
function ODE(dz, z, θ, t)
    dz[1] = O_NET(z, θ, st)[1][1]
    dz[2] = O_NET(z, θ, st)[1][2]
end
\end{minted}

However, in Structured Neural ODE, we endow the ODE with a known part $\dot{q}_t = p_t/m$. And the unknown part $\dot{p}_t = -q_t/c$ is now replaced by the output of a neural network $\dot{p}_t = f_{\theta}(q_t)$, where the neural network takes $q_t$ as its input.

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
m = 2
function ODE(dz, z, θ, t)
    q = z[1]
    p = z[2]
    # the time derivative of q is a known part
    dz[1] = p/m
    # the time derivative of p is an unknown part
    dz[2] = Structured_ODE_NN([q], θ, st)[1][1]
end
\end{minted}

In the above case, we hypothesize that $\Dot{p}_t$ is only affected by $q_t$ (in fact, the real equation is $\dot{p}_t = -q_t/c$). Thus, the neural network $f_{\theta}$ is a 1-input and 1-output model. If we are unsure of what the input is, we can simply use the entire state variable $\mathbf{z}_t = (q_t, p_t)$ as input and construct a 2-input and 1-output model for it. Nevertheless, in any case, the input must contain $q_t$, e.g. $q_t^2$, $(q_t-1)$, $(q_t, p_t)$, etc. A good guess can greatly improve the efficiency of training (in this case, $q_t$ is a good guess and $-q_t/c$ is the best). 

A very good guess (assume that we already know the spring compliance is around 4, 4 is probably imprecise but close enough) could be like:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
m = 2
c = 4
function ODE(dz, z, θ, t)
    q = z[1]
    p = z[2]
    dz[1] = p/m
    dz[2] = Structured_ODE_NN([-q/c], θ, st)[1][1]
end
\end{minted}

In the opposite, if $q_t$ does not appear in the input, e.g. only $p_t$ as the input, the model training will not converge in all probability. Such a bad guess could be like:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
m = 2
function ODE(dz, z, θ, t)
    q = z[1]
    p = z[2]
    dz[1] = p/m
    dz[2] = Structured_ODE_NN([p], θ, st)[1][1]
end
\end{minted}

\section{Experiment: Undamped Harmonic Oscillator}
In this experiment, we continue using an undamped harmonic oscillator as example and compare the models trained with Neural ODEs and with structured Neural ODEs.

We use the Adam algorithm to train the model with the learning rate $0.001$. First we construct an neural IVP like \ref{eq:structured_neura_IVP} with the initial state $\mathbf{z}_0 = [1.0, 1.0]$. The training set is generated by solving the system of ODEs with implicit midpoint method. The evolution of the dynamics begins from $0.0$ to $19.9$ with the time step size of $0.1$. This is a discrete trajectory with 200 time steps in the training set. Following the above example, we suppose that the mass $m=2$ is known, while the spring compliance $c$ is unknown. Thus, the dynamics can be given by

\begin{equation}
    \label{eq:NeuralODE_udho}
    \begin{aligned}
        \dot{q}&=\frac{p}{m},\\
        \dot{p}&=f_{\theta}(q),
    \end{aligned}
\end{equation}

where $f_{\theta}$ is a structured ODE neural network.

To evaluate the models, we generate test set by taking 100 time steps from $0.0$ to $9.9$ with the time step size of $0.1$. With the optimized parameters, we can obtain the estimated trajectories $\{ \mathbf{\hat{z}}_{t} \}_{t=1}^{T} = \{ (\hat{q}_{t}, \hat{p}_{t}) \}_{t=1}^{T}$ and plot the phase portrait of the dynamcis predicted by H-NET and HNN:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/phase_portrait_H_NET_and_HNN.pdf}
    \caption{The phase portrait of the dynamcis predicted by H-NET and HNN.}
    \label{fig:phase_portrait_H_NET_and_HNN}
\end{figure}

Similarly, we also plot the phase portrait of the dynamcis predicted by O-NET and structured ODE neural network.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/phase_portrait_O_NET_and_structured_ODE_NN.pdf}
    \caption{The phase portrait of the dynamcis predicted by O-NET and structured ODE neural network.}
    \label{fig:phase_portrait_O_NET_and_structured_ODE_NN}
\end{figure}

As we can see, both H-NET and HNN learned better dynamcis of the undamped hamonic oscillator in the experiment.

We can also evaluate the models from other perpectives. The prediction error is the $L_{2}$ error over time.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/prediction_error_H_NET_and_HNN.pdf}
    \caption{The prediction error of H-NET and HNN.}
    \label{fig:prediction_error_H_NET_and_HNN}
\end{figure}

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/prediction_error_O_NET_and_structured_ODE_NN.pdf}
    \caption{The prediction error of O-NET and structured ODE neural network.}
    \label{fig:prediction_error_O_NET_and_structured_ODE_NN}
\end{figure}

In a Hamiltonian system, the total energy of the system should be conserved. We can also plot the evolution of the total energy.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/Hamiltonian_evolution_H_NET_and_HNN.pdf}
    \caption{The time evolution of the total energy predicted by H-NET and HNN within (0.0, 10.0).}
    \label{fig:Hamiltonian_evolution_H_NET_and_HNN}
\end{figure}

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/Hamiltonian_evolution_O_NET_and_structured_ODE_NN.pdf}
    \caption{The time evolution of the total energy predicted by O-NET and structured ODE neural network within (0.0, 10.0).}
    \label{fig:Hamiltonian_evolution_O_NET_and_structured_ODE_NN}
\end{figure}

At the first sight, the evolutions of the total energy in above figures are restricted within a certain range. However, if we extend the time step to 100:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/Hamiltonian_evolution_long_H_NET_and_HNN.pdf}
    \caption{The time evolution of the total energy predicted by H-NET and HNN within (0.0, 100.0).}
    \label{fig:Hamiltonian_evolution_long_H_NET_and_HNN}
\end{figure}

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/Hamiltonian_evolution_long_O_NET_and_structured_ODE_NN.pdf}
    \caption{The time evolution of the total energy predicted by O-NET and structured ODE neural network within (0.0, 100.0).}
    \label{fig:Hamiltonian_evolution_long_O_NET_and_structured_ODE_NN}
\end{figure}

Both H-NET and HNN still preserve the total energy pretty good, while the prediction of O-NET started to diverge. We also noticed that structured ODE neural network may perform better than O-NET in this experiment.


\clearpage
\section{Experiment: Isothermal Damped Harmonic Oscillator}
An isothermal damped harmonic oscillator is a port-Hamiltonian system, in which the mechanical energy of the system dissipates with the vibration of the damping. H-NET and HNN are designed to learn a quantity that obeys the conservation law. As the mechanical energy of a port-Hamiltonian system may not be conserved. Hence, it may not make sense to continue using H-NET or HNN in this experiment. In fact, the authors of HNNs proposed D-HNNs (Dissipative Hamiltonian Neural Networks) to handle this situation in 2022 \cite{greydanus2022dissipative}. However, in this experiment, we will only focus on O-NET and structured ODE neural network.

For an isothermal damped harmonic oscillator, the dynamics is given by

\begin{equation}
    \label{eq:ODE_idho}
    \begin{bmatrix}
    \dot{q}\\
    \\
    \dot{p}\\
    \\
    \dot{s}\\
    \end{bmatrix}
    =
    \begin{bmatrix}
    \frac{p}{m}\\
    \\
    -\frac{q}{c}-d\frac{p}{m}\\
    \\
    \frac{1}{\theta_{0}} d v^2\\
    \end{bmatrix}.
\end{equation}

Suppose that the terms involving damping are unknown, we can rewrite the system of ODEs as

\begin{equation}
    \label{eq:NeuralODE_idho}
    \begin{bmatrix}
    \dot{q}\\
    \\
    \dot{p}\\
    \\
    \dot{s}\\
    \end{bmatrix}
    =
    \begin{bmatrix}
    \frac{p}{m}\\
    \\
    -\frac{q}{c} + f_{\theta}(v)[1]\\
    \\
    f_{\theta}(v^2)[2]\\
    \end{bmatrix},
\end{equation}

where the output of the neural network $f_{\theta}(\cdot)$ is a vector with two components. We use $f_{\theta}(v)[1]$ to express the first component and $f_{\theta}(v^2)[2]$ to express the second component.

In this experiment, we set the initial state $\mathbf{z}_0 = [1.0, 1.0, 0.2]$, the mass $m = 2$, the spring compliance $c = 1$, the damping coefficient $d=0.5$ and the environment temperature $\theta_0=300$. We generate the training set from $0.0$ to $9.9$ with the time step size of $0.1$. The system of structured Neural ODEs is solved by the implicit midpoint method. For optimization, we use the Adam algorithm with the learning rate $0.001$.

To evaluate the model, we plot the phase portrait from the prediction and compare it with the ground truth.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/phase_portrait_idho_O_NET_and_structured_ODE_NN.pdf}
    \caption{The phase portrait of the dynamcis predicted by O-NET and structured ODE neural network.}
    \label{fig:phase_portrait_idho_O_NET_and_structured_ODE_NN}
\end{figure}

The following is the prediction error over time.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/prediction_error_idho_O_NET_and_structured_ODE_NN.pdf}
    \caption{The prediction error of O-NET and structured ODE neural network.}
    \label{fig:prediction_error_idho_O_NET_and_structured_ODE_NN}
\end{figure}

The mechanical energy in this experiment is not conserved, as we can see in the figure below:

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/Hamiltonian_evolution_idho_O_NET_and_structured_ODE_NN.pdf}
    \caption{The time evolution of the mechanical energy predicted by O-NET and structured ODE neural network within (0.0, 100.0).}
    \label{fig:Hamiltonian_evolution_idho_O_NET_and_structured_ODE_NN}
\end{figure}


\clearpage
\chapter{Compositional Modelling}
\label{ch:chapter7}

As stated in the Port-Hamiltonian Systems chapter, a classical port-Hamiltonian system, e.g. an isothermal damped harmonic oscillator, can be depicted by a bond-graph expression \ref{fig:bondgraph_idho}. In \cite{lohmayer2022exergetic} and \cite{lohmayer2022ephs}, a thermodynamic modelling framework is proposed to extend a classical port-Hamiltonian system to an exergetic port-Hamiltonian system (EPHS). This modelling framework combines the classical port-Hamiltonian structure and the GENERIC framework, such that the EPHS is coherent with both the first and the second law of thermodynamics.

For an isothermal damped harmonic oscillator, its EPHS model can by expressed by the following bond-graph expression:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/bondgraph_idho_exergetic.pdf}
    \caption{Bond-graph expression for defining an EPHS model of an isothermal damped harmonic oscillator.}
    \label{fig:bondgraph_idho_exergetic}
\end{figure}

However, following the assumption in the preceding chapter, there may be an unknown part in the system, which could be an obstacle to modeling. In this chapter, we propose a component-based modelling approach based on the EPHS framework that uses neural networks to learn the subsystems (or system components). This approach combines with machine learning techniques and provides a direction for compositional grey-box modelling.

For example, a realistic damping model could be difficult to obtain. Hence, we suppose that the resistive structure in red is the unknow part, a structured ODE neural network $\mathtt{f_{\theta}}$ can be substituted for it:

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/bondgraph_idho_exergetic_NN.pdf}
    \caption{Bond-graph expression for defining an EPHS model of an isothermal damped harmonic oscillator with neural network.}
    \label{fig:bondgraph_idho_exergetic_NN}
\end{figure}

The entire model is a grey-box model, which is composed by the known components (storage components, Dirac structure and isothermal environment) and the unknown component (resistive structure). However, the unknown component replaced with a neural network is a pure black box model. Such a black box model without EPHS structure is not guaranteed to satisfy the first and the second law of thermodynamics.


\section{Substitution}

Recall that the resistive structure in \ref{eq:resistive_structure_idho} is given by the relation

\begin{equation}
    \label{eq:resistive_structure_interconnection_idho}
    \left[\begin{array}{l}\mathtt{R_{d}.p.f} \\ \mathtt{R_{d}.s_{e}.f}\end{array}\right]=\frac{1}{\theta_0} d\left[\begin{array}{rr}\theta_0+\mathtt{R_{d}.s_{e}.e} & -\mathtt{R_{d}.p.e} \\ -\mathtt{R_{d}.p.e} & \frac{(\mathtt{R_{d}.p.e})^2}{\theta_0+\mathtt{R_{d}.s_{e}.e}}\end{array}\right]\left[\begin{array}{l}\mathtt{R_{d}.p.e} \\ \mathtt{R_{d}.s_{e}.e}\end{array}\right].
\end{equation}

As in Figure \ref{fig:bondgraph_idho_exergetic_NN}, we treat the damping as a black box. Then, the environment temperature $\theta_0$ and the effort variables $\left[\begin{array}{l}\mathtt{R_{d}.p.e} \\ \mathtt{R_{d}.s_{e}.e}\end{array}\right]$ can be considered as inputs to the black box. We replace the RHS with a neural network $\mathtt{f_{\theta}}$ and rewrite Equation \ref{eq:resistive_structure_interconnection_idho} as

\begin{equation}
    \label{eq:resistive_structure_interconnection_NN_idho}
    \left[\begin{array}{l}\mathtt{f_{\theta}.p.f} \\ \mathtt{f_{\theta}.s_{e}.f}\end{array}\right]= \mathtt{f_{\theta}} \left(\theta_0, 
    \left[\begin{array}{l}\mathtt{R_{d}.p.e} \\ \mathtt{R_{d}.s_{e}.e}\end{array}\right] \right),
\end{equation}

where the port $\mathtt{f_{\theta}.p}$ with the port variables $(\mathtt{f_{\theta}.p.f}, \mathtt{R_{d}.p.e}) = (dv, v)$ is connected to the Dirac structure $\mathtt{D_m}$ and the port $\mathtt{f_{\theta}.s_{e}}$ with the port variables $(\mathtt{f_{\theta}.s_{e}.f}, \mathtt{R_{d}.s_{e}.e}) = (-\frac{1}{\theta_{0}}dv^2, \theta_{d}-\theta_{0})$ is connected to the isothermal environment $\mathtt{E}$. Since the oscillator was assumed to be isothermal, the damping temperature $\theta_{d}$ is equal to the envirnoment temperature $\theta_{0}$, i.e., $\mathtt{R_{d}.s_{e}.e} = \theta_{d}-\theta_{0} = 0$.

To obtain the system dynamics, we solve structured Neural ODEs (or a system of structured Neural ODEs)

\begin{equation}
    \label{eq:NeuralODE_idho_EPHS}
    \begin{bmatrix}
    \dot{q}\\\\
    \dot{p}\\\\
    \dot{s_e}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \frac{p}{m}\\\\
    -\frac{q}{c}-\mathtt{f_{\theta}.p.f}\\\\
    -\mathtt{f_{\theta}.s_{e}.f}
    \end{bmatrix},
\end{equation}

where $s_e$ is the environment entropy.


\section{Modelling with structured Neural ODEs}

Similar to \ref{eq:structured_neura_IVP}, we also construct an IVP for the above structured Neural ODEs \ref{eq:NeuralODE_idho_EPHS}

\begin{equation}
    \label{eq:structured_neural_EPHS_IVP}
    \dot{\mathbf{z}}_t = h_{\theta}(f_{\theta}, \theta_0, \mathbf{z}_t), \quad \mathbf{z}(t_{0}) = \mathbf{z}_{0},
\end{equation}

where $\mathbf{z}_t$ is the state variable $(q_t, p_t, s_{e,t})$ at time $t$ and $h_{\theta}$ is a physics informed function. Note that $h_{\theta}$ differs from $\mathtt{f_{\theta}}$ in \ref{eq:resistive_structure_interconnection_NN_idho}. While the value of $\mathtt{f_{\theta}}$ is $ \left[\begin{array}{l}\mathtt{f_{\theta}.p.f} \\ \mathtt{f_{\theta}.s_{e}.f}\end{array}\right]$, the value of $h_{\theta}$ is the time derivative of the state variable $\dot{\mathbf{z}}_t = \begin{bmatrix}\dot{q_t}\\ \dot{p_t}\\ \dot{s_{e,t}}\end{bmatrix}$. We can observe the relation between $\mathtt{f_{\theta}}$ and $h_{\theta}$ via \ref{eq:NeuralODE_idho_EPHS}.

In Julia code, we construct structured Neural ODEs like:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
m = 2
c = 1
θ_0 = 300
function StructuredNeuralODE(df, z, θ, t)
    q, p, s_e = z
    v = p/m
    dz[1] = v
    dz[2] = -q/c - NN([v], θ, st)[1][1]
    dz[3] = - NN([-v^2/θ_0], θ, st)[1][2]
end
\end{minted}

We construct a 1-input and 2-output neural network and use $v$ and $\theta_0$ as inputs to the neural network for estimating the flow variables $\left[\begin{array}{l}\mathtt{R_{d}.p.f} \\ \mathtt{R_{d}.s_{e}.f}\end{array}\right]$. After the training, We obatin the output of the neural network $\mathtt{f_{\theta}.p.f}$ via

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
NN([v], θ, st)[1][1]
\end{minted}

and $\mathtt{f_{\theta}.s_{e}.f}$ via

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
NN([-v^2/θ_0], θ, st)[1][2]
\end{minted}


\clearpage
\section{Experiment: Isothermal Damped Harmonic Oscillator}
In this experiment, we set the mass $m=2$, the spring compliance $c=1$ and the environment temperature $\theta_{0}=300$ for the structured Neural ODEs. For generating a training set, we set the mass $m=2$, the spring compliance $c=1$, the damping coefficient $d=0.5$ and the environment temperature $\theta_{0}=300$. We construct an IVP the same as \ref{eq:structured_neural_EPHS_IVP} with the initial state $\mathbf{z}_{0} = [1.0, 1.0, 0.2]$. The training set begins from $0.0$ to $19.9$ with the time step size of $0.1$. We train the model using Adam algorithm with the learning rate 0.0001.

\begin{figure}[h!]
    \centering
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/phase_portrait_compositional_idho.pdf}
    \\(a)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/prediction_error_compositional_idho.pdf}
    \\(b)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Hamiltonian_evolution_compositional_idho.pdf}
    \\(c)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/p.f_compositional_idho.pdf}
    \\(d)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/se.f_compositional_idho.pdf}
    \\(e)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipated_power_damping_idho.pdf}
    \\(f)
    \end{minipage}
    \caption{The experiment results: (a) The phase portrait of the dynamcis predicted by structured ODE neural network. (b) The prediction error of structured ODE neural network. (c) The time evolution of the mechanical energy predicted by structured ODE neural network within (0.0, 100.0). (d) The time evolution of the flow variable $\mathtt{p.f}$. (e) The time evolution of the flow variable $\mathtt{s_{e}.f}$. (f) The time evolution of the dissipated power at the damping.}
    \label{fig:idho_experiment}
\end{figure}

Figure \ref{fig:idho_experiment}(a) is the phase portrait with taking the spring displacement $q$ as x-axis and the mass momentum $p$ as y-axis.

We train the neural network from $0.0$ to $19.9$, while we test the neural network from $0.0$ to $99.9$. Hence, it is not surprising that the prediction error in Figure \ref{fig:idho_experiment}(b) begins to diverge after a certain time.

The mechanical energy of the system dissipates over time. With the vibration of the oscillator, eventually all the mechanical energy will be transformed into thermal energy. Therefore, the mechanical energy will eventually fall to zero. We also observed this process in Figure \ref{fig:idho_experiment}(c).

Since we use a neural network to replace the resistive structure, we hypothesize that it is critical to evaluate the model by the time evolution of the flow variables of the resistive structure. In Figure \ref{fig:idho_experiment}(d), the blue line (Ground Truth) represents the time evolution of $\mathtt{R_{d}.p.f}$ (damping force), while the orange line (Structured ODE NN) represents the time evolution of $\mathtt{f_{\theta}.p.f}$ (damping force predicted by the neural network $\mathtt{f_{\theta}}$). The ground truth and the prediction of the time evolution of $\mathtt{s_e.f}$ (entropy rate) are also shown in Figure \ref{fig:idho_experiment}(e).

We also evaluate the model by the time evolution of the dissipated power at the damping. With the vibration of the oscillator, the mechanical energy of the system becomes less and less, and the dissipated power will eventually fall to zero. We can observe this phenomenon in Figure \ref{fig:idho_experiment}(f).

As stated before, we expect that the trained neural network models can be reused for other systems. We first store all the models and the corresponding parameters on disk. Then, we load them and set the mass $m=4$ ($m=2$ before). We also change the initial state to be $\mathbf{z}_{0} = [1.0, 2.0, 0.2]$ ($\mathbf{z}_{0} = [1.0, 1.0, 0.2]$ before), while the other setups remain the same as before.

\begin{figure}[h!]
    \centering
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/phase_portrait_compositional_idho_reuse.pdf}
    \\(a)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/prediction_error_compositional_idho_reuse.pdf}
    \\(b)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Hamiltonian_evolution_compositional_idho_reuse.pdf}
    \\(c)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/p.f_compositional_idho_reuse.pdf}
    \\(d)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/se.f_compositional_idho_reuse.pdf}
    \\(e)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipated_power_damping_idho_reuse.pdf}
    \\(f)
    \end{minipage}
    \label{fig:idho_reuse}
    \caption{The results of reusing trained models: (a) The phase portrait of the dynamcis predicted by the reused models. (b) The prediction error of the reused models. (c) The time evolution of the mechanical energy predicted by the reused models. (d) The time evolution of the flow variable $\mathtt{p.f}$ predicted by the reused models. (e) The time evolution of the flow variable $\mathtt{s_e.f}$ predicted by the reused models. (f) The time evolution of the dissipated power at the damping predicted by the reused models.}
\end{figure}


\clearpage
\section{Experiment: Non-isothermal Damped Harmonic Oscillator}
In this experiment, we model a non-isothermal damped harmonic oscillator with a thermal capacity at the damping. In contrast to the isothermal model, the temperature difference between the damping and the environment $\Delta\theta = \theta_d - \theta_0$ affects the thermal conduction in the non-isothermal model. The thermal conduction is also affected by a coefficient $\alpha$. For details about this EPHS model, we refer to \cite{lohmayer2021exergetic}.

The EPHS model of a non-isothermal damped harmonic oscillator can be expressed by  the following bond-graph expression: 

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/bondgraph_ndho_exergetic.pdf}
    \caption{Bond-graph expression for defining an EPHS model of an non-isothermal damped harmonic oscillator.}
    \label{fig:bondgraph_ndho_exergetic}
\end{figure}

Comparing to Figure \ref{fig:bondgraph_idho_exergetic}, the damping in Figure \ref{fig:bondgraph_ndho_exergetic} is modeled as a composition of a resistive structure (the damping) $\mathtt{R_d}$, a storage component (thermal capacity of the damping) $\mathtt{H_d}$ and a resistive structure (thermal conduction) $\mathtt{R_{tc}}$. 

The resistive structures $\mathtt{R_d}$ and $\mathtt{R_{tc}}$ can be given by the relations

\begin{equation}
    \label{eq:resistive_structure_damping_interconnection_ndho}
    \left[\begin{array}{l}\mathtt{R_{d}.p.f} \\ \mathtt{R_{d}.s_{e}.f}\end{array}\right]=\frac{1}{\theta_0} d\left[\begin{array}{rr}\theta_0+\mathtt{R_{d}.s_{e}.e} & -\mathtt{R_{d}.p.e} \\ -\mathtt{R_{d}.p.e} & \frac{(\mathtt{R_{d}.p.e})^2}{\theta_0+\mathtt{R_{d}.s_{e}.e}}\end{array}\right]\left[\begin{array}{l}\mathtt{R_{d}.p.e} \\ \mathtt{R_{d}.s_{e}.e}\end{array}\right],
\end{equation}

and

\begin{equation}
    \label{eq:resistive_structure_thermal_conduction_interconnection_ndho}
    \left[\begin{array}{l}\mathtt{R_{tc}.s_{d}.f} \\ \mathtt{R_{tc}.s_{e}.f}\end{array}\right]=\frac{1}{\theta_0} \alpha \left[\begin{array}{rr} \frac{\theta_0+\mathtt{R_{tc}.s_{e}.e}}{\theta_0+\mathtt{R_{tc}.s_{d}.e}} & -1 \\ -1 & \frac{\theta_0+\mathtt{R_{tc}.s_{d}.e}}{\theta_0+\mathtt{R_{tc}.s_{e}.e}}\end{array}\right]\left[\begin{array}{l}\mathtt{R_{tc}.s_{d}.e} \\ \mathtt{R_{tc}.s_{e}.e}\end{array}\right].
\end{equation}

The box $\mathtt{R_d}$ has two ports $\mathtt{R_{d}.p}$ and $\mathtt{R_{d}.s_{d}}$, where the port variables are $(\mathtt{R_{d}.p.f}, \mathtt{R_{d}.p.e}) = (dv, v)$ and $(\mathtt{R_{d}.s_{d}.f}, \mathtt{R_{d}.s_{d}.e}) = (-\frac{1}{\theta_d}dv^2, \theta_d - \theta_0)$.

The box $\mathtt{R_{tc}}$ has two ports $\mathtt{R_{tc}.s_{d}}$ and $\mathtt{R_{tc}.s_{e}}$, where the port variables are $(\mathtt{R_{tc}.s_{d}.f}, \mathtt{R_{tc}.s_{d}.e}) = (\frac{1}{\theta_d}\alpha(\theta_d-\theta_0), \theta_d-\theta_0)$ and $(\mathtt{R_{tc}.s_{e}.f}, \mathtt{R_{tc}.s_{e}.e}) = (\frac{1}{\theta_0}\alpha(\theta_0-\theta_d), \theta_0-\theta_0)$.

Similarly, we assume that the system components damping and thermal conduction are unknown (or partially unknown) and replace them with two neural networks $\mathtt{f_{\theta, d}}$ and $\mathtt{g_{\theta, tc}}$:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures/bondgraph_ndho_exergetic_NN.pdf}
    \caption{Bond-graph expression for defining an EPHS model of an non-isothermal damped harmonic oscillator with neural networks.}
    \label{fig:bondgraph_ndho_exergetic_NN}
\end{figure}

And we rewrite the relations as

\begin{equation}
    \label{eq:resistive_structure_damping_interconnection_NN_ndho}
    \left[\begin{array}{l}\mathtt{f_{\theta}.p.f} \\ \mathtt{f_{\theta, d}.s_{d}.f}\end{array}\right]=\mathtt{f_{\theta,d}} \left(\theta_0, 
    \left[\begin{array}{l}\mathtt{R_{d}.p.e} \\ \mathtt{R_{d}.s_{d}.e}\end{array}\right] \right),
\end{equation}

and

\begin{equation}
    \label{eq:resistive_structure_thermal_conduction_interconnection_NN_ndho}
    \left[\begin{array}{l}\mathtt{g_{\theta, tc}.s_{d}.f} \\ \mathtt{g_{\theta}.s_{e}.f}\end{array}\right]=\mathtt{g_{\theta,tc}} \left(\theta_0, 
    \left[\begin{array}{l} \mathtt{R_{tc}.s_{d}.e} \\ \mathtt{R_{tc}.s_{e}.e}\end{array}\right] \right).
\end{equation}

To obtain the system dynamics, we solve a system of ODEs with numerical method. For a non-isothermal damped harmonic oscillator with neural network, the system of structured Neural ODEs is of the form:

\begin{equation}
    \label{eq:NeuralODE_ndho_EPHS}
    \begin{bmatrix}
    \dot{q}\\\\
    \dot{p}\\\\
    \dot{s_e}\\\\
    \dot{s_d}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \frac{p}{m}\\\\
    -\frac{q}{c}-\mathtt{f_{\theta}.p.f}\\\\
    \mathtt{g_{\theta}.s_{e}.f}\\\\
    -\mathtt{f_{\theta, d}.s_{d}.f}-\mathtt{g_{\theta, tc}.s_{d}.f}
    \end{bmatrix}.
\end{equation}

In Julia code, we construct the system of structured Neural ODEs like:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
m = 2
c = 1
θ_0 = 300
c_tc = 1
function StructuredNeuralODE(dz, z, θ, t)
    q, p, s_e, s_d = z
    v = p/m
    θ_d = exp(s_d/c_tc) / c_tc
    Δθ = θ_d - θ_0
    θ_d = θ.θ_d
    θ_tc = θ.θ_tc   
    dz[1] = v
    dz[2] = - q/c - NN_d([v], θ_d, st_d)[1][1]
    dz[3] = - NN_tc([Δθ/θ_0], θ_tc, st_tc)[1][2]
    dz[4] = - NN_d([-(v^2)/θ_d], θ_d, st_d)[1][2] - NN_tc([Δθ/θ_d], θ_tc, st_tc)[1][1]
end
\end{minted}

In the experiment, we set the mass $m=2$, the spring compliance $c=1$, the environment temperature $\theta_{0}=300$ and the heat capacity $c_{tc}=1$ for the structured Neural ODEs. For generating a training set, we set the mass $m=2$, the spring compliance $c=1$, the damping coefficient $d=0.5$, the environment temperature $\theta_{0}=300$, the heat transfer coefficient $\alpha=0.2$ and the heat capacity $c_{tc}=1$. We construct an IVP with the ODEs \ref{eq:NeuralODE_ndho_EPHS} and the initial state $\mathbf{z}_{0} = [1.0, 1.0, 0.2, 5.8]$. The training set begins from $0.0$ to $19.9$ with the time step size of $0.1$. We train the model using the Adam algorithm with the learning rate 0.0001.

\clearpage
\begin{figure}[h!]
    \centering
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/phase_portrait_compositional_ndho.pdf}
    \\(a)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/prediction_error_compositional_ndho.pdf}
    \\(b)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Hamiltonian_evolution_compositional_ndho.pdf}
    \\(c)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/p.f_compositional_ndho.pdf}
    \\(d)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/sd_d.f_compositional_ndho.pdf}
    \\(e)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/sd_tc.f_compositional_ndho.pdf}
    \\(f)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/se.f_compositional_ndho.pdf}
    \\(g)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipative_power_damping_ndho.pdf}
    \\(h)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipative_power_thermal_conduction_ndho.pdf}
    \\(i)
    \end{minipage}
    \caption{The experiment results: (a) The phase portrait of the dynamcis predicted by structured ODE neural network. (b) The prediction error of structured ODE neural network. (c) The time evolution of the mechanical energy predicted by structured ODE neural network. (d) The time evolution of the flow variable $\mathtt{p.f}$. (e) The time evolution of the flow variable $\mathtt{s_{d}.f}$ of the damping. (f) The time evolution of the flow variable $\mathtt{s_{d}.f}$ of the thermal conduction. (g) The time evolution of the flow variable $\mathtt{s_e.f}$. (h) The time evolution of the dissipated power at the damping. (i) The time evolution of the dissipated power at the thermal conduction.}
    \label{fig:ndho_experiment}
\end{figure}

We can observe that the time evolution of the flow variable $\mathtt{s_{d}.f}$ of the damping in Figure \ref{fig:ndho_experiment}(e) and $\mathtt{s_{d}.f}$ of the thermal conduction in Figure \ref{fig:ndho_experiment}(f) differ from the ground truth by a bias. In addition, the ground truth trajectories in both figures eventually converge to zero, while the predicted trajectories do not. As stated before, a black box model without EPHS structure are not guaranteed to satisfy the first and the second law of thermodynamics. 

In this experiment, the neural network models are learned from the state trajectories, and the structured Neural ODE associated with these two flow variables is

\begin{equation}
    \label{eq:NeuralODE_ndho_EPHS_last_line}
    \dot{s_d} = -\mathtt{f_{\theta, d}.s_{d}.f}-\mathtt{f_{\theta, tc}.s_{d}.f}.
\end{equation}

We can observe that the RHS of this Structured Neural ODE is the negative sum of two (predicted) flow variables, but with this information alone, we cannot learn each term separately and accurately. We address this issue in the next experiment.

\clearpage
\section{Experiment: Non-isothermal Damped Harmonic Oscillator with EPHS Structure}
In this experiment, the neural network models are used to replace the damping coefficient and the heat transfer coefficient. Normally, the realistic models are nonlinear. In this case, we hypothesize that the damping coefficient is the value of a nonlinear function $d \left( \theta_0, \left[\begin{array}{l}\mathtt{R_{d}.p.e} \\ \mathtt{R_{d}.s_{d}.e}\end{array}\right] \right)$ depending on the environment temperature and the effort variables. Similarly, the heat transfer coefficient is the value of a nonlinear function $\alpha \left( \theta_0, \left[\begin{array}{l} \mathtt{R_{tc}.s_{d}.e} \\ \mathtt{R_{tc}.s_{e}.e}\end{array}\right] \right)$. This approach endows the structured Neural ODEs with EPHS structure of the irreversible components. We hypothesize that this approach allows the neural networks to converge easier and learn better.

In Julia code, we construct the system of structured Neural ODEs like:

\begin{minted}[breaklines,escapeinside=||,mathescape=true, numbersep=3pt, gobble=2, frame=lines, fontsize=\small, framesep=2mm]{julia}
m = 2
c = 1
θ_0 = 300
c_tc = 1
function StructuredNeuralODE(dz, z, θ, t)
    q, p, s_e, s_d = z
    v = p/m
    θ_d = exp(s_d/c_tc) / c_tc
    Δθ = θ_d - θ_0
    θ_NN_d = θ.θ_NN_d
    θ_NN_tc = θ.θ_NN_tc   
    dz[1] = v
    dz[2] = - q/c - NN_d([v, θ_d], θ_NN_d, st_d)[1][1]*v
    dz[3] = NN_tc([θ_d, θ_0], θ_NN_tc, st_tc)[1][1]*(Δθ)/θ_0
    dz[4] = NN_d([v, θ_d], θ_NN_d, st_d)[1][1]*((v)^2)/θ_d - NN_tc([θ_d, θ_0], θ_NN_tc, st_tc)[1][1]*(Δθ)/θ_d
end
\end{minted}

We use the same setups as in the previous experiment and obtain the following results:

\clearpage
\begin{figure}[h!]
    \centering
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/phase_portrait_compositional_ndho_with_EPHS_structure.pdf}
    \\(a)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/prediction_error_compositional_ndho_with_EPHS_structure.pdf}
    \\(b)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Hamiltonian_evolution_compositional_ndho_with_EPHS_structure.pdf}
    \\(c)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/p.f_compositional_ndho_with_EPHS_structure.pdf}
    \\(d)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/sd_d.f_compositional_ndho_with_EPHS_structure.pdf}
    \\(e)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/sd_tc.f_compositional_ndho_with_EPHS_structure.pdf}
    \\(f)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/se.f_compositional_ndho_with_EPHS_structure.pdf}
    \\(g)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipative_power_damping_ndho_with_EPHS_structure.pdf}
    \\(h)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipative_power_thermal_conduction_ndho_with_EPHS_structure.pdf}
    \\(i)
    \end{minipage}
    \caption{The experiment results: (a) The phase portrait of the dynamcis predicted by structured ODE neural network. (b) The prediction error of structured ODE neural network. (c) The time evolution of the mechanical energy predicted by structured ODE neural network. (d) The time evolution of the flow variable $\mathtt{p.f}$. (e) The time evolution of the flow variable $\mathtt{s_{d}.f}$ of the damping. (f) The time evolution of the flow variable $\mathtt{s_{d}.f}$ of the thermal conduction. (g) The time evolution of the flow variable $\mathtt{s_e.f}$. (h) The time evolution of the dissipated power at the damping. (i) The time evolution of the dissipated power at the thermal conduction.}
    \label{fig:ndho_experiment_with_EPHS_structure}
\end{figure}

To evaluate the model more comprehensively, we change the mass to be $m=4$ ($m=2$ before) and the initial state to be $\mathbf{z}_{0} = [1.0, 2.0, 0.2, 5.8]$ ($\mathbf{z}_{0} = [1.0, 1.0, 0.2, 5.8]$ before), while the other setups remain the same as before.

\begin{figure}[h!]
    \centering
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/phase_portrait_compositional_ndho_with_EPHS_structure_reuse.pdf}
    \\(a)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/prediction_error_compositional_ndho_with_EPHS_structure_reuse.pdf}
    \\(b)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Hamiltonian_evolution_compositional_ndho_with_EPHS_structure_reuse.pdf}
    \\(c)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/p.f_compositional_ndho_with_EPHS_structure_reuse.pdf}
    \\(d)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/sd_d.f_compositional_ndho_with_EPHS_structure_reuse.pdf}
    \\(e)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/sd_tc.f_compositional_ndho_with_EPHS_structure_reuse.pdf}
    \\(f)
    \end{minipage}
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/se.f_compositional_ndho_with_EPHS_structure_reuse.pdf}
    \\(g)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipative_power_damping_ndho_with_EPHS_structure_reuse.pdf}
    \\(h)
    \end{minipage}%
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/dissipative_power_thermal_conduction_ndho_with_EPHS_structure_reuse.pdf}
    \\(i)
    \end{minipage}
    \caption{The results of reusing trained models: (a) The phase portrait of the dynamcis predicted by reused models. (b) The prediction error of the reused models. (c) The time evolution of the mechanical energy predicted by the reused models. (d) The time evolution of the flow variable $\mathtt{p.f}$ predicted by the reused models. (e) The time evolution of the flow variable $\mathtt{s_{d}.f}$ of the damping predicted by the reused models. (f) The time evolution of the flow variable $\mathtt{s_{d}.f}$ of the thermal conduction predicted by the reused models. (g) The time evolution of the flow variable $\mathtt{s_e.f}$ predicted by the reused models. (h) The time evolution of the dissipated power at the damping predicted by the reused models. (i) The time evolution of the dissipated power at the thermal conduction predicted by the reused models.}
    \label{fig:ndho_experiment_with_EPHS_structure_reuse}
\end{figure}

\clearpage
\chapter{Conclusion}
\label{ch:chapter8}
\section{Summary}
This thesis aims to combine machine learning techniques with the EPHS modelling framework and provide a direction to compositional grey-box modelling. Normally, a system can be considered as a composition of subsystem components. In the case where some components are known and some components are unknown, we replaced the unknown components with neural network models, making the whole model a grey-box model. In addition, these trained neural network models can be reused as subsystems of other systems.

From Chapter 2 to 5, we reviewed fundamental background knowledge for port-Hamiltonian systems modelling and machine learning. 

In Chapter 6, we first compared various neural networks based on Neural ODEs. Then we explained structured Neural ODEs, which are Neural ODEs endowed with structure. This structure composes the known parts and unknown parts. In the field of physics, the known parts can be some known physics laws, while the unknown parts can be replaced by the outputs of neural networks.

In Chapter 7, we took an isothermal damped harmonic oscillator and a non-isothermal damped harmonic oscillator as examples and use structured Neural ODEs to model these two systems. In the experiment of section 7.3, we assumed that the damping is an unknown component and replaced it with a neural network. The neural network took the environment temperature and the effort variables (velocity, temperature difference) as inputs, while the outputs of the neural network are the flow variables (damping force, entropy rate). From the results, we found that the trained neural network can provide good predictions. Thereafter, we changed the parameter (mass) and the initial state (momentum) to build a new system and reused the neural network model for the new system. In the new system, the predictions were still good. In the experiment of section 7.4, we assumed that both the damping and the thermal conduction are unknown components and replaced each of them with a neural network. The inputs to the neural networks are the environment temperature and the effort variables, while the output are the flow variables. However, the results of this experiment did not turn out well. As we treated these two unknown components as black boxes, the predictions are not guaranteed to obey the first and second laws of thermodynamics. Moreover, in the case where there are more than one neural network model in the system, we did not provide enough constraints. To address this issue, in the experiment of section 7.5, we endowed the structured Neural ODEs with the complete EPHS structure implying the first and second laws of thermodynamics. We assumed that the damping and the thermal conduction coefficients are nonlinear and can be learned by neural networks. We also adopted the environment temperature and the effort variables as inputs to the neural networks. This time, the experimental results look quite good and can also be reused for a new system.

\section{Outlook}
Future work may focus on providing a more general approach and endowing the neural networks with more structural properties (in case the complete EPHS structure are not entirely known), such that the predictions are coherent with the first and second laws of thermodynamics. 

So far, the studied systems in this thesis were restricted to harmonic oscillators. To verify the generality of the approaches proposed in this thesis, these approaches should be tested on other systems. 

Furthermore, it would be interesting to add noise to the training data to verify the robustness of these approaches.

Finally, since this thesis is the first step of our work in terms of system identification, there is still a lot that can be done. I hope that the approaches and experiments in this thesis can provide a potential direction for future work.

%%%%%%%%%%%%%%
% REFERENCES %
%%%%%%%%%%%%%%
\bibliographystyle{apalike}
\renewcommand{\bibname}{References} % Title References instead of Bibliography
\bibliography{literature/literature}


%%%%%%%%%%%%
% APPENDIX %
%%%%%%%%%%%%
\appendix


\end{document}